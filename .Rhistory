# https://doi.org/10.1038/ng.3667"
doi <- "https://doi.org/10.1038/ng.3667"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
library(openalexR)
#library(dplyr)
library(tidyverse)
library(writexl)
# free unused obj to manage memory
rm(list=ls())
gc
options("max.print" = 100000)
options (openalexR.mailto="yhan@arizona.edu")
getwd()
setwd("/home/yhan/Documents/openalexR-test/")
org_works_2023 <- readRDS("../org_works_2023.rds")
##### 2. Checking and verifying data
###### change this line only to update the right dataset.
ref_works <- org_works$referenced_works
### 2.1 Checking Column referenced_works:  a list
class(ref_works)
# Find "NA" indexes
na_indices <- which(sapply(ref_works, function(x) is.logical(x) && is.na(x)))
# count how many "NA" in referenced_works col. ~ 15%-20%  of works contain "NA"
na_count <- sum(sapply(ref_works, function(x) is.logical(x) && is.na(x)))
print(na_count)
### 2.2 Combine all the references and do further data analysis
# Avg # of references per article: 38
# Year 2022 total references: 345,904
ref_works_combined <- unlist(ref_works, use.names = FALSE)
ref_works_combined <- ref_works_combined[!is.na(ref_works_combined)]  # Remove NA values
summary(ref_works_combined)
### 2.21 finding these duplicates, which mean the duplicates have been cited multiple times
# (probably more important to have these journals subscribed!)
# cited more: ~20% - 25%  (2022, 2023 UArizona data)
ref_works_more_cited <- ref_works_combined[duplicated(ref_works_combined)]
table(ref_works_more_cited)
##### 2. Checking and verifying data
###### change this line only to update the right dataset.
ref_works <- org_works$referenced_works
# change working data here
org_works <- org_works_2023
##### 2. Checking and verifying data
###### change this line only to update the right dataset.
ref_works <- org_works$referenced_works
### 2.1 Checking Column referenced_works:  a list
class(ref_works)
# Find "NA" indexes
na_indices <- which(sapply(ref_works, function(x) is.logical(x) && is.na(x)))
# count how many "NA" in referenced_works col. ~ 15%-20%  of works contain "NA"
na_count <- sum(sapply(ref_works, function(x) is.logical(x) && is.na(x)))
print(na_count)
### 2.2 Combine all the references and do further data analysis
# Avg # of references per article: 38
# Year 2022 total references: 345,904
ref_works_combined <- unlist(ref_works, use.names = FALSE)
ref_works_combined <- ref_works_combined[!is.na(ref_works_combined)]  # Remove NA values
summary(ref_works_combined)
### 2.21 finding these duplicates, which mean the duplicates have been cited multiple times
# (probably more important to have these journals subscribed!)
# cited more: ~20% - 25%  (2022, 2023 UArizona data)
ref_works_more_cited <- ref_works_combined[duplicated(ref_works_combined)]
table(ref_works_more_cited)
############################################################
### 2.23 For Testing purpose: Trace back from the cited article -> $referenced_works -> original published article
# Find the index of multiple samples
which(ref_works_combined %in% c("https://openalex.org/W4292309267", "https://openalex.org/W621173178"))
# Find the index of "ref1" in the published works' referenced_works.
which(sapply(org_works$referenced_works, function(x) "https://openalex.org/W4292309267" %in% x))
# We can see the original works for samples
org_works[706, "id"]
org_works[779, "id"]
ref_works_cited <- unique(ref_works_combined)
summary(ref_works_cited)
##### 3. Data cleanup.
# Flattening authors fields from the DF (multiple authors per work).
# 426,000 obs (multiple authors) from 50,400 obs (works)
org_works_since <- org_works
#### Year 2022:
# -- org_works_authors: 75,222
# -- org_works_UAauthors: 16,432
# -- Org_ref_works_combined: 656,712
# -- Org_ref_works_cited: 249,629
#
org_works_authors<-org_works_since%>%
mutate(author=lapply(author, function(x){
names(x) <-paste0(names(x), "author")
return(x)
}))%>%
unnest(author)
# After flattening, authors' fields (e.g. au_idauthor, institution_rorauthor) are displayed
colnames(org_works)
colnames(org_works_authors)
#### This is not 100% accurate because UArizona has child organization whose ROR is associated with an article. By filtering institution_rorauthor
# to UArizona's ROR, certain articles are left out!!!
# 2024-09: I am currently working with openAlexR developers to fix this.
org_works_UAauthors <- org_works_authors%>%filter(institution_rorauthor== "https://ror.org/03m2x1q45")
org_works_UAauthors_unique <- unique (org_works_UAauthors)
# 3.4 check work_cited
org_ref_works <- org_works_UAauthors$referenced_works
class(org_ref_works) # a list
# Combine all the references and do further data analysis.
# Giving avg ~ 38 references per work, the number of all the references'  will be 38x of the works.
org_ref_works_combined <- unlist(org_ref_works, use.names = FALSE)
org_ref_works_combined <- org_ref_works_combined[!is.na(org_ref_works_combined)]  # Remove NA values
# finding these duplicates, which mean the duplicates have been cited multiple times
# Cited multiple times = 65%
org_ref_works_more_cited <- org_ref_works_combined[duplicated(org_ref_works_combined)]
# 2.22 remove the duplicates for further processing. unique works cited ~38%
org_ref_works_cited <- unique(org_ref_works_combined)
# Find the indices of elements matching a pattern
matching_indices <- grep("https://openalex.org/W4401226694", org_ref_works)
print(matching_indices)  # Returns the index(es) of matching elements
matching_indices <- grep("https://openalex.org/W3203440266", ref_works )
print(matching_indices)  # Returns the index(es) of matching elements
#Running the loop to retrieve works cited data (may take someref_works_cited2#Running the loop to retrieve works cited data (may take some time to run)
# 1,000 for testing ONLY
num_of_works <- 1000
# the real number of works cited
# 2023-current: 379,978
# 2022-current: 571,227. Note: This crashed R studio with 12 GB memeroy used. showing 560,000
num_of_works <- length(org_ref_works_cited)
### Testing if a work is found.
search_string <- "https://openalex.org/W2919115771"
result <- lapply(org_ref_works, function(x) grep(search_string, x, value = TRUE))
print(result)
matches <- result[sapply(result, length) > 0]
print(matches)
indices <- which(sapply(org_ref_works, function(x) any(grepl(search_string, x))))
for (i in indices) {
cat("Index:", i, "\n")
cat("Element:\n", org_ref_works[[i]], "\n\n")
}
#Creating an empty dataframe to store the results of the for loop.
works_cited_df <-data.frame()
Works_Cited2_df <- works_cited_df
fetch_number <- 100
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (i in range_i){
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(identifier=batch_identifiers)
works_cited_ls[[i]] <- batch_data
}
})
View(works_cited_ls)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (i in range_i){
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(identifier=batch_identifiers)
works_cited_ls[[i]] <- batch_data
}
})
View(works_cited_ls)
works_cited_df <- readRDS("../works_cited_2023.rds")
articles_cited_df <- works_cited_df
# 1. Analyse journal usage
#  - remove any row whose col "issn_l" is empty or NULL
#  - 2023: 224,572 articles out of 241,752 works
#  - 2022: 226,471 articles out of 243,452 works
articles_cited_df <- articles_cited_df[!(is.na(articles_cited_df$issn_l) | articles_cited_df$issn_l == ""), ]
nrow(articles_cited_df)
saveRDS(articles_cited_df, "../articles_cited_df_2023.rds")
# Trim and normalize the host_organization column
articles_cited_df$host_organization <- trimws(articles_cited_df$host_organization)
articles_cited_df$issn_l <- trimws(articles_cited_df$issn_l)
# Empty or NULL records
count_null_empty_id <- sum(is.na(articles_cited_df$id) | trimws(articles_cited_df$id) == "")
count_null_empty_id
articles_cited_df <- articles_cited_df[!(is.na(articles_cited_df$id) | trimws(articles_cited_df$id) == ""), ]
# publisher: host_organization
unique_publishers <- unique(articles_cited_df$host_organization)
# number of publishers: ~1,600
num_unique_publishers <- length(unique_publishers)
# list top 50 publishers
print(unique_publishers[1:50])
# list NULL publishers = 5%
num_na <- sum(is.na(articles_cited_df$host_organization))
# Replace NA values and empty strings with "NA"
articles_cited_df$host_organization[is.na(articles_cited_df$host_organization) | trimws(articles_cited_df$host_organization) == ""] <- "NA"
# 1. First, showing all NA publisher: meaning publisher info is not available.
publisher_NA <- articles_cited_df[articles_cited_df$host_organization == "NA", ]
publisher_NA_id <-unique(publisher_NA$id)
# Check if any 'id' values are duplicated
any_duplicated_ids <- any(duplicated(publisher_NA$id))
# Not using unnect() because it flattens out every article per author, which creates a lot of duplicated info
library(jsonlite)
# Convert the 'author' dataframe to JSON for each row
publisher_NA <- publisher_NA %>%
mutate(author = sapply(author, function(x) toJSON(x)))
# Truncate only strings that exceed Excel's 32,767 character limit
publisher_NA <- publisher_NA %>%
mutate(across(where(is.character), ~ ifelse(nchar(.) > 32767, substr(., 1, 32767), .)))
# 2. Elsevier
publisher_elsevier <- articles_cited_df[articles_cited_df$host_organization == "Elsevier BV", ]
# 3. Springer
publisher_springer <- articles_cited_df[articles_cited_df$host_organization == "Springer Science+Business Media", ]
# Some of open source publishers
publisher_plos <-articles_cited_df[articles_cited_df$host_organization == "Public Library of Science", ]
write_xlsx(publisher_plos, "publisher_plos_2023.xlsx")
publisher_aaas <-articles_cited_df[articles_cited_df$host_organization == "American Association for the Advancement of Science", ]
write_xlsx(publisher_aaas, "publisher_aaas_2023.xlsx")
publisher_nature <-articles_cited_df[articles_cited_df$host_organization == "Nature Portfolio", ]
publisher_cdc <- articles_cited_df[articles_cited_df$host_organization == "Centers for Disease Control and Prevention", ]
publisher_ua <- articles_cited_df[articles_cited_df$host_organization == "University of Arizona", ]
publisher_uap <- articles_cited_df[articles_cited_df$host_organization == "University of Arizona Press", ]
# Group by 'host_organization' and count the number of articles for each publisher
publisher_ranking <- articles_cited_df %>%
group_by(host_organization) %>%
summarise(article_count = n()) %>%
arrange(desc(article_count))
library(ggplot2)
top_20_publishers <- publisher_ranking %>% slice(1:20)
# Bar plot for top 20 publishers
ggplot(top_20_publishers, aes(x = reorder(host_organization, -article_count), y = article_count)) +
geom_bar(stat = "identity", fill = "steelblue") +
geom_text(aes(label = article_count), vjust = 0.5, hjust = -0.2) +  # Add count labels on the bars
coord_flip() +  # Flip the axis for better readability
labs(x = "Publisher", y = "Number of Articles", title = "2023 UA Top 10 Publishers (Number of Articles Cited)") +
theme_minimal()
# Bar plot for top 20 publishers
ggplot(top_20_publishers, aes(x = reorder(host_organization, -article_count), y = article_count)) +
geom_bar(stat = "identity", fill = "steelblue") +
geom_text(aes(label = article_count), vjust = 0.5, hjust = -0.2) +  # Add count labels on the bars
coord_flip() +  # Flip the axis for better readability
labs(x = "Publisher", y = "Number of Articles", title = "2023 UA Top 20 Publishers (Number of Articles Cited)") +
theme_minimal()
fetch_number <- 100
num_of_works2 <- 5000
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
num_of_works <- 5000
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (i in range_i){
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(identifier=batch_identifiers)
works_cited_ls[[i]] <- batch_data
}
})
print(paste("fetch time: ", time_taken["elapsed"] / 60, "minutes"))
time_taken <- system.time ({
works_cited2_df <- rbindlist(works_cited_ls, use.names=TRUE, fill=TRUE)
})
print(paste("rbind time: ", time_taken["elapsed"] / 60, "minutes"))
time_taken2 <- system.time ({
works_cited2_df <- rbindlist(works_cited_ls, use.names=TRUE, fill=TRUE)
})
print(paste("rbind time: ", time_taken2["elapsed"] / 60, "minutes"))
# fetch_number = 1,000, 161 mins
library(data.table)
time_taken2 <- system.time ({
works_cited2_df <- rbindlist(works_cited_ls, use.names=TRUE, fill=TRUE)
})
print(paste("rbind time: ", time_taken2["elapsed"] / 60, "minutes"))
num_of_works <- 50000
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (i in range_i){
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(identifier=batch_identifiers)
works_cited_ls[[i]] <- batch_data
}
})
print(paste("fetch time: ", time_taken["elapsed"] / 60, "minutes"))
fetch_number <- 100
num_of_works <- 10000
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (i in range_i){
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(entity="works", verbose = TRUE, identifier=batch_identifiers)
works_cited_ls[[i]] <- batch_data
}
})
print(paste("fetch time: ", time_taken["elapsed"] / 60, "minutes"))
fetch_number <- 100
num_of_works <- 1000
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (i in range_i){
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(entity="works", verbose = TRUE, identifier=batch_identifiers, options= LIST(sample=10000, seed=1), output="list")
works_cited_ls[[i]] <- batch_data
}
})
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (i in range_i){
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(entity="works", verbose = TRUE, identifier=batch_identifiers,
options= list(sample=10000, seed=1), output="list")
works_cited_ls[[i]] <- batch_data
}
})
print(paste("fetch time: ", time_taken["elapsed"] / 60, "minutes"))
num_of_works <- 10000
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (i in range_i){
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(entity="works", verbose = TRUE, identifier=batch_identifiers,
options= list(sample=10000, seed=1), output="list")
works_cited_ls[[i]] <- batch_data
}
})
print(paste("fetch time: ", time_taken["elapsed"] / 60, "minutes"))
malaria_topic <- oa_fetch(entity = "topics", search = "malaria") %>%
filter(display_name == "Malaria") %>%
pull(id)
malaria_topic
# Scalar fields
select_fields <- c("id", "cited_by_count", "display_name")
system.time({
res <- oa_fetch(
topics.id = malaria_topic,
entity = "works",
verbose = TRUE,
options = list(sample = 10000, seed = 1,
select = select_fields),
output = "list"
)
})
View(res)
# Scalar fields
#select_fields <- c("id", "cited_by_count", "display_name")
system.time({
res <- oa_fetch(
topics.id = malaria_topic,
entity = "works",
verbose = TRUE,
options = list(sample = 10000, seed = 1,),
#                  select = select_fields),
output = "list"
)
})
# Scalar fields
#select_fields <- c("id", "cited_by_count", "display_name")
system.time({
res <- oa_fetch(
topics.id = malaria_topic,
entity = "works",
verbose = TRUE,
options = list(sample = 10000, seed = 1),
#                  select = select_fields),
output = "list"
)
})
View(res)
system.time({
res <- oa_fetch(
topics.id = malaria_topic,
entity = "works",
#verbose = TRUE,
options = list(sample = 10000, seed = 1),
output = "list"
)
})
system.time({
res <- oa_fetch(
topics.id = malaria_topic,
entity = "works",
verbose = TRUE,
options = list(sample = 1000, seed = 1),
output = "list"
)
})
fetch_number <- 1000
num_of_works <- 1000
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (i in range_i){
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(entity="works", verbose = TRUE, identifier=batch_identifiers,
options= list(sample=1000, seed=1), output="list")
works_cited_ls[[i]] <- batch_data
}
})
print(paste("fetch time: ", time_taken["elapsed"] / 60, "minutes"))
fetch_number <- 1000
num_of_works <- 10000
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (idx in range_i){
i <- range_i[idx]
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(entity="works", verbose = TRUE, identifier=batch_identifiers,
options= list(sample=1000, seed=1), output="list")
works_cited_ls[[idx]] <- batch_data
}
})
fetch_number <- 1000
num_of_works <- 5000
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (idx in range_i){
i <- range_i[idx]
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(entity="works", verbose = TRUE, identifier=batch_identifiers,
options= list(sample=1000, seed=1), output="list")
works_cited_ls[[idx]] <- batch_data
}
})
fetch_number <- 1000
num_of_works <- 5000
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (idx in seq_along(range_i)) {
i <- range_i[idx]
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(entity="works", verbose = TRUE, identifier=batch_identifiers,
options= list(sample=1000, seed=1), output="list")
works_cited_ls[[idx]] <- batch_data
}
})
print(paste("fetch time: ", time_taken["elapsed"] / 60, "minutes"))
View(works_cited_ls)
fetch_number <- 10000
num_of_works <- 10000
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (idx in seq_along(range_i)) {
i <- range_i[idx]
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(entity="works", verbose = TRUE, identifier=batch_identifiers,
options= list(sample=10000, seed=1), output="list")
works_cited_ls[[idx]] <- batch_data
}
})
print(paste("fetch time: ", time_taken["elapsed"] / 60, "minutes"))
fetch_number <- 1000
num_of_works <- 1000
system.time({
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(entity="works", identifier=batch_identifiers,
options= list(sample=10000, seed=1), output="list")
#  works_cited_ls[[idx]] <- batch_data
})
system.time({
batch_identifiers <-org_ref_works_cited[0,999]
batch_data <-oa_fetch(entity="works", identifier=batch_identifiers,
options= list(sample=10000, seed=1), output="list")
#  works_cited_ls[[idx]] <- batch_data
})
system.time({
batch_identifiers <-org_ref_works_cited[1:999]
batch_data <-oa_fetch(entity="works", identifier=batch_identifiers,
options= list(sample=10000, seed=1), output="list")
#  works_cited_ls[[idx]] <- batch_data
})
batch_identifiers <-org_ref_works_cited[1:1000]
system.time({
batch_identifiers <-org_ref_works_cited[1:1000]
batch_data <-oa_fetch(entity="works", identifier=batch_identifiers,
options= list(sample=1000, seed=1), output="list")
})
system.time({
batch_identifiers <-org_ref_works_cited[1:1000]
res <-oa_fetch(identifier=batch_identifiers,
entity = "works",
options= list(sample=1000, seed=1),
output="list")
})
########################################
### Testing optimization of rbind and oa_fetch
### 10,000 works in 76 seconds
malaria_topic <- oa_fetch(entity = "topics", search = "malaria") %>%
filter(display_name == "Malaria") %>%
pull(id)
malaria_topic
system.time({
res <- oa_fetch(
topics.id = malaria_topic,
entity = "works",
verbose = TRUE,
options = list(sample = 1000, seed = 1),
output = "list"
)
})
########################################
### Testing optimization of rbind and oa_fetch
### 10,000 works in 76 seconds
malaria_topic <- oa_fetch(entity = "topics", search = "malaria") %>%
filter(display_name == "Malaria") %>%
pull(id)
malaria_topic
system.time({
res <- oa_fetch(
topics.id = malaria_topic,
entity = "works",
verbose = TRUE,
options = list(sample = 10000, seed = 1),
output = "list"
)
})
