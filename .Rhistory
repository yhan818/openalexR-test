test_data_affiliation <- c("University of Arizona")
test_data_year <- c("2022", "2021", "2020", "2012")
# Test works
works_from_dois <- oa_fetch(entity = "works", doi = c("https://doi.org/10.1093/ofid/ofac186", "https://doi.org/10.1007/s11192-013-1221-3"),  verbose = TRUE)
### Testing three datasets citations recall and precision using one article (published in 2022)
### OpenAlex: Precision
### OpenCitaitons: Precision 100%. Recall: 2/3
### Google scholar: Precision 100%. Recall 100%
works_from_dois <- oa_fetch(entity = "works", doi = c("https://doi.org/10.6017/ital.v40i1.12553"),  verbose = TRUE)
works_from_dois$cited_by_api_url
works_from_dois$ids
# All locations:
# count: 14903 (2024-07-11)
UA_host_all_location <- oa_fetch (
entity = "works",
locations.source.host_organization = "https://openalex.org/I138006243",
#count_only = TRUE
)
View(source_counts_df)
# common libraries
library(openalexR)
packageVersion("openalexR")
library(dplyr)
library(ggplot2)
library(knitr)
library(testthat)
library(readr)
citation("openalexR")
# check to see if openAlexR has the latest entities in OpenAlex (OpenAlex updated its data model(Entities) in June 2023)
# Before April 2023: they are [1] "works"        "authors"      "venues"       "institutions" "concepts"
# If not, need to use openalexR developer's version
oa_entities()
options (openalexR.mailto="yhan@arizona.edu")
getwd()
setwd("/home/yhan/Documents/openalexR-test")
### Test data
test_data_UAL_authors     <- c("Yan Han", "Ellen Dubinski", "Fernando Rios", "Ahlam Saleh")
test_data_COM_authors     <- c("Phillip Kuo", "Bekir Tanriover", "Ahlam Saleh")
test_data_COM_Tucson_authors <- c("Che Carrie Liu", "Robert M. Aaronson", "Alexa Aasronson", "Mohammed Abbas", "")
test_data_science_authors <- c("Marek Rychlik", "Ali Bilgin", "Beichuan Zhang")
test_data_ischool_authors <- c("Hong Cui")
test_data_others          <- c("Leila Hudson", "Mona Hymel")
test_data_not_updated_authors <-c("Karen Padilla", "Haw-chih Tai")
test_data_affiliation <- c("University of Arizona")
test_data_year <- c("2022", "2021", "2020", "2012")
# Best OA location. find out host organization.
# count: 8394 (2024-07-11)
# This fetch will take a few minutes. So be patient .
UA_host_best_location <- oa_fetch(
entity = "works",
# UA campus repository ID does not work as a filter
best_oa_location.source.host_organization = "https://openalex.org/I138006243",
# If only need count, uncomment the below line for a quick run.
count_only = TRUE
# If only need some samples. using the below line.
# options = list(sample = 100, seed = 1)
)
# Primary_location.source.host_organization.
# count: 24 (2024-07-11)
UA_host2 <- oa_fetch (
entity = "works",
primary_location.source.host_organization = "https://openalex.org/I138006243",
count_only = TRUE
)
############### Use campus repository source ID.
# no result using UA campus repository source ID.
UA_host5 <- oa_fetch (
entity = "works",
best_oa_location.source.host_organization = "https://openalex.org/S2764879211",
count_only = TRUE
)
UA_host6 <- oa_fetch (
entity = "works",
locations.source.host_organization = "https://openalex.org/S2764879211",
count_only = TRUE
)
# Filter the dataframe to get all rows where "so" is "journal of range management" (JRM) (case insenstive)
df_jrm <- subset(UA_host_all_location, grepl("journal of range management", so, ignore.case = TRUE))  #4,183
df_rem <- subset(UA_host_all_location, grepl("Range Ecology", so, ignore.case = TRUE)) # 432
# Count the number of occurrences of each unique value in the "source" column using dplyr
source_counts_df <- UA_host_all_location %>%
count(so, sort = TRUE)
# Display the dataframe with counts
# JRM/REM:  4183+836 (3991 + 804 for best_oa)
# Rangelands: 782 (685 for best_oa)
# NA: 384
print(source_counts_df)
#### Example 1: More like openalex pulled directly from crossref.
###  Campus repo:
doi <- "10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
library(rcrossref)
library (httr)
library(jsonlite)
# Function to get metadata for a DOI
get_doi_metadata <- function(doi) {
url <- paste0("https://api.crossref.org/works/", doi)
response <- GET(url)
if (status_code(response) == 200) {
metadata <- content(response, as = "text", encoding = "UTF-8")
metadata <- fromJSON(metadata, flatten = TRUE)
return(metadata$message)
} else {
message("Error retrieving metadata: ", status_code(response))
return(NULL)
}
}
#### Example 1: More like openalex pulled directly from crossref.
###  Campus repo:
doi <- "10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
### Example 3:
# https://doi.org/10.1038/ng.3667"
doi <- "https://doi.org/10.1038/ng.3667"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
ls
### 1.2 Getting all the works based on the institution ROR and publication date. It takes longer time.
# see above for the running time
org_works_2019 <-oa_fetch(
entity="works",
institutions.ror=c("03m2x1q45"),
from_publication_date ="2019-01-01",
to_publication_date = "2019-12-31"
)
install.packages("openalexR")
install.packages("remotes")
remotes::install_github("ropensci/openalexR", force=TRUE)
library(openalexR)
packageVersion("openalexR")
library(dplyr)
library(tidyverse)
library(data.table)
library(writexl)
# free unused obj to manage memory
gc()
rm(list=ls())
gc()
options("max.print" = 100000)
options (openalexR.mailto="yhan@arizona.edu")
getwd()
setwd("/home/yhan/Documents/openalexR-test/")
# Typically only some seconds
UAworks_count <-oa_fetch(
entity="works",
institutions.ror=c("03m2x1q45"),
from_publication_date ="2022-01-01",
to_publication_date = "2022-12-31",
#primary_location.source.type = "journal",
count_only = TRUE
)
works_published_2022 <-oa_fetch(
entity="works",
institutions.ror=c("03m2x1q45"),
from_publication_date ="2022-01-01",
to_publication_date = "2022-12-31",
# primary_location.source.type = "journal"
)
saveRDS(works_published_2022, "../works_published_2022.rds")
# SHALL get all works, then filter them if needed.
# 2023: All works: 9,384 without type =journal (2024-09)
# 2023: All works: 10,559 (2025-01)
# 2023: journal only: 6,903 using primary_location.source.type = "journal" as a filter (not including type="repository")
#
works_published_2023 <-oa_fetch(
entity="works",
institutions.ror=c("03m2x1q45"),
from_publication_date ="2023-01-01",
to_publication_date = "2023-12-31",
#primary_location.source.type = "journal"
)
saveRDS(works_published_2023, "../works_published_2023.rds")
works_published <- works_published_2022
##########################################################################################
#### Testing the later fetched dataset and comparing it with the previous fetched data
names(works_published)
print(class(works_published$id))
print(length(works_published$id))
str(works_published_old)
any(is.na(works_published$id))
##### 2. Checking and verifying data
##### 2.1 Route 1: Getting citation data from $referenced_works
##### Route 2: Getting author's data?
###### change this line only to update the right dataset.
works_published_ref <- works_published$referenced_works
# There are NA references. So we need to remove them.
# ~14% works have no references (2021)
# This na_indices include type: article, books, errata, letter, and other types
na_indices <- which(sapply(works_published_ref, function(x) is.logical(x) && is.na(x)))
na_count <- sum(sapply(works_published_ref, function(x) is.logical(x) && is.na(x)))
na_percent <- na_count/length(works_published_ref) * 100
# Remove duplicate rows from the data frame
unique_works_published <- unique(works_published)
works_published_ref <- unique(works_published_ref) # this actually also remove NA lists.
# rm(works_published_ref_combined)
works_published_ref_combined <- unlist(works_published_ref, use.names = FALSE)
works_published_ref_combined <- works_published_ref_combined[!is.na(works_published_ref_combined)]  # Remove NA values
############################################################
### 2.23 For Testing purpose: Trace back from the cited article -> $referenced_works -> original published article
# Find the index of multiple samples
head(works_published$referenced_works)
head(works_published_ref_unique)
# Use sapply to find matching elements in the works_published_ref for testing.
matching_indices <- which(sapply(works_published_ref, function(x)
any(x %in% c("https://openalex.org/W1624352668", "https://openalex.org/W1548779692")))) # https://openalex.org/W1624352668 were cited on 2021 and 2023 data
# We can see the original works for samples
works_published[2, "id"]
works_published[174, "id"]
# Use sapply to find matching elements in the works_published_ref for testing.
matching_indices <- which(sapply(works_published_ref_combined, function(x)
any(x %in% c("https://openalex.org/W1624352668", "https://openalex.org/W1548779692")))) # https://openalex.org/W1624352668 were cited on 2021 and 2023 data
print(matching_indices)
#####################################################
#################### 3.3 TESTING!!!#################
####################################################
# Then extract UArizona authors only
# 94,500 obs from 426,000 obs (UA authors only).
## https://openalex.org/A5033317672 Saurav Mallik (is at two affiliations for https://api.openalex.org/works/W4389611927. Harvard and University of Arizona)
### https://openalex.org/W4401226694 author Renu Malhotra has two affiliations.
oa_fetch_test1 <-oa_fetch( entity="works",  id="https://openalex.org/W4401226694")
oa_fetch_test1$author
view(oa_fetch_test1[[4]][[1]])
oa_fetch_test2 <-oa_fetch( entity="authors",  id="https://openalex.org/A5003933592")
### 3.33 Testing if a cited work is found.
# Deep Learning, Nature, by Yann LeCun, Yoshua Bengio, Geoffrey Hinton. Cited by: 62,210
search_string <- "https://openalex.org/W2919115771"
result <- lapply(works_published_ref_combined, function(x) grep(search_string, x, value = TRUE))
matches <- result[sapply(result, length) > 0]
# Find it from the original article
search_string <- "https://openalex.org/W2594545996"
# this article was cited 81 (2019, 130 (2020), 90 (2021), 52 (2022), 16 (2023)
indices_with_string <- which(sapply(works_published$referenced_works, function(x) search_string %in% x))
print(indices_with_string)
works_published[indices_with_string, ]$id
# test case 2: cited 6 from microbiology, multiple times for 2019, 2020, 2021, 2022
# both final published version and pre-print existing: https://openalex.org/works/W4379795917 and https://openalex.org/W4319339791
search_string <- "https://openalex.org/W2128159409"
indices_with_string <- which(sapply(works_published$referenced_works, function(x) search_string %in% x))
print(indices_with_string)
works_published[indices_with_string, ]$id
#Creating an empty dataframe to store the results of the for loop.
works_cited <-data.frame()
fetch_number <- 100
num_of_works <- 10000
fetch_number <- 50
num_of_works <- length (works_published_ref_combined)
fetch_number <- 50
#num_of_works <-1000
num_of_works <- length (works_published_ref_combined)
works_cited <- readRDS("../works_cited_2022.rds")
# Filter rows where issn_l is neither NA nor an empty string
works_cited_source_issn_index <- !is.na(works_cited$issn_l) & works_cited$issn_l != ""
# Subset
works_cited_source_issn <- works_cited[works_cited_source_issn_index, ]
# Filter rows where issn_l is neither NA nor an empty string
works_cited_source_issn_index <- !is.na(works_cited$issn_l) & works_cited$issn_l != ""
# Subset
works_cited_source_issn <- works_cited[works_cited_source_issn_index, ]
# Subset non_articles_cited (the rest)
works_cited_source_nonissn <- works_cited[!works_cited_source_issn_index, ]
works_cited_source_issn_articles    <- works_cited_source_issn[works_cited_source_issn$type == "article", ]
works_cited_source_issn_nonarticles <- works_cited_source_issn[works_cited_source_issn$type != "article", ]
works_cited_source_nonissn_articles    <- works_cited_source_nonissn[works_cited_source_nonissn$type == "article", ]
works_cited_source_nonissn_nonarticles <- works_cited_source_nonissn[works_cited_source_nonissn$type != "article", ]
# Empty or NULL records
count_null_empty_id <- sum(is.na(works_cited_source_issn$id) | trimws(works_cited_source_issn$id) == "")
count_null_empty_id
# publisher: host_organization
unique_publishers <- unique(works_cited_source_issn$host_organization)
# number of publishers: ~1,600
num_unique_publishers <- length(unique_publishers)
# list top 50 publishers
print(unique_publishers[1:50])
# 2022: 3,312 NA / 323,221
# 2021: 3,687 NA / 341,738
# 2020: 4,039 NA / 382,495
num_na <- sum(is.na(works_cited_source_issn$host_organization))
# list NULL publishers ~ 1 %
# 2023: 2,227 (probably need ISSN matching) / 2,922 NA/
# 2022: 3,312 NA / 323,221
# 2021: 3,687 NA / 341,738
# 2020: 4,039 NA / 382,495
num_na <- sum(is.na(works_cited_source_issn$host_organization))
# Replace NA values and empty strings with "NA"
works_cited_source_issn$host_organization[is.na(works_cited_source_issn$host_organization) | trimws(works_cited_source_issn$host_organization) == ""] <- "NA"
# Dealing with "NA" data in "host_organization" field.
# 1. First, showing all NA publisher: meaning publisher info is not available.
publisher_NA <- works_cited_source_issn[works_cited_source_issn$host_organization == "NA", ]
publisher_NA_id <-unique(publisher_NA$id)
# Check if any row in the df 'publisher_NA' contains a non-missing value in the "issn_l" column
publisher_NA_with_issn <- publisher_NA[!is.na(publisher_NA$`issn_l`) & publisher_NA$`issn_l` != "", ]
print(publisher_NA_with_issn)
# Extract unique ISSNs from the 'issn_l' column: 1235 unique issns
# 2023: 1,236 / 3,489 NA
# 2022: 1,110 / 3,312 NA
# 2021: 1,204 / 3,687 NA
# 2020: 1,737 / 4,039 NA
unique_issn <- unique(publisher_NA$`issn_l`)
print(unique_issn)
# Not using unnect() because it flattens out every article per author, which creates a lot of duplicated info
library(jsonlite)
# Convert the 'author' dataframe to JSON for each row
publisher_NA <- publisher_NA %>%
mutate(author = sapply(author, function(x) toJSON(x)))
# Truncate only strings that exceed Excel's 32,767 character limit
publisher_NA <- publisher_NA %>%
mutate(across(where(is.character), ~ ifelse(nchar(.) > 32767, substr(., 1, 32767), .)))
View(publisher_NA)
publisher_name <- "Microbiology society"
publisher_microbiology <- works_cited_source_issn[grepl(publisher_name, works_cited_source_issn$host_organization_name, ignore.case = TRUE), ]
publisher_microbiology <- works_cited_source_issn[grepl(publisher_name, works_cited_source_issn$host_organization, ignore.case = TRUE), ]
publisher_elsevier <- works_cited_source_issn[grepl("Elsevier BV", works_cited_source_issn$host_organization, ignore.case = TRUE), ]
# 2025-02:  Brill (https://openalex.org/publishers/p4310320561)
publisher_brill  <- works_cited_source_issn[grepl("Brill", works_cited_source_issn$host_organization, ignore.case = TRUE), ]
View(publisher_brill)
publisher_brill2 <- works_cited_source_nonissn[grepl("Brill", works_cited_source_nonissn$host_organization, ignore.case = TRUE), ]
View(publisher_brill2)
publisher_brill <- publisher_brill %>%
mutate(across(where(is.character), ~ ifelse(nchar(.) > 32767, substr(., 1, 32767), .)))
write_xlsx(publisher_bmj, "citations/works_cited_source_issn_brill_2022.xlsx")
write_xlsx(publisher_brill, "citations/works_cited_source_issn_brill_2022.xlsx")
View(publisher_brill)
publisher_bmj  <- works_cited_source_issn[grepl("BMJ", works_cited_source_issn$host_organization, ignore.case = TRUE), ]
publisher_bmj2 <- works_cited_source_nonissn[grepl("BMJ", works_cited_source_nonissn$host_organization, ignore.case = TRUE), ]
publisher_bmj <- publisher_bmj %>%
mutate(across(where(is.character), ~ ifelse(nchar(.) > 32767, substr(., 1, 32767), .)))
write_xlsx(publisher_bmj, "citations/works_cited_source_issn_bmj_2022.xlsx")
fetch_number <- 50
works_cited2 <-data.frame()
# Loop to fetch data in batches
time_taken <- system.time({
for(i in seq(1, num_of_works, by = fetch_number)) {
batch_identifiers <- works_published_ref_combined[i:min(i + fetch_number - 1, num_of_works)]
# Check if the batch_identifiers is a valid vector
if (length(batch_identifiers) > 0 && !all(is.na(batch_identifiers))) {
# Fetch data from OpenAlex using oa_fetch, ensure proper identifier input
batch_data <- tryCatch({
# Have to use "primary_location.source.type = journal" to filter out non-journal.
# issn_l cannot be used alone (there are book chapters which have issn per OpenAlex)
oa_fetch(identifier = batch_identifiers)
#, primary_location.source.type = "journal")
}, error = function(e) {
message("Error fetching data: ", e)
return(NULL)
})
# Only bind non-null data
if (!is.null(batch_data) && nrow(batch_data) >0 ) {
# Ensure consistent columns
batch_data <- data.table::setDT(batch_data)[, setdiff(names(works_cited), names(batch_data)) := NA]
works_cited2 <- rbindlist(list(works_cited, batch_data), use.names = TRUE, fill = TRUE)
}
}
}
})
print(time_taken)
setdiff(works_cited, works_cited2)
setdiff(works_cited2, works_cited)
saveRDS(works_cited2, "../works_cited_2022.rds")
###########################################
### Search if a publisher is in a DF
# Output the publisher
# @ return: the indices of the publisher
search_publisher <- function(publisher_string, df) {
# Find indices where the host_organization contains the publisher string (case insensitive)
indices_with_string <- which(grepl(publisher_string, df$host_organization, ignore.case = TRUE))
print(df[indices_with_string, ]$host_organization)
print(df[indices_with_string, ]$id)
return(indices_with_string)
}
# Example usage:
publisher_string <- "Brill"
result_indices <- search_publisher(publisher_string, works_cited_source_issn)
# Print the indices
print(result_indices)
#### Function: search_work_publisher():
## Search a work's publisher and output the publisher
# @return: index of the DF
search_work_publisher <- function(search_string, df) {
# Find indices where the host_organization contains the search string (case insensitive)
indices_with_string <- which(sapply(df$id, function(x) !is.na(x) && search_string %in% x))
print(df[indices_with_string, ]$host_organization)
print(indices_with_string)
return(indices_with_string)
}
# Example usage:
search_string <- "https://openalex.org/W2944198613"
result_indices <- search_work_publisher(search_string, works_published)
###############################################################
# Verify any cited work using the function search_references()
# Define the function to search for a string in the referenced_works column and print the output
##############################################3
search_references <- function(search_string, df) {
indices_with_string <- which(sapply(df$referenced_works, function(x) search_string %in% x))
print(indices_with_string)
print(df[indices_with_string, ]$id)
}
# Example usage:
search_string <- "Emerald Publishing"
result_indices <- search_publisher(search_string, works_published)
print(result_indices)
search_string <- "Brill"
result_indices <- search_publisher(search_string, works_published)
search_string <- "https://openalex.org/W2944198613"
#search_string <- "https://openalex.org/W3216054981"
indices_with_string <- which(sapply(works_published$referenced_works, function(x) search_string %in% x))
search_references(search_string, works_published)
search_string <- "https://openalex.org/W2465933872"
indices_with_string <- which(sapply(works_published$referenced_works, function(x) search_string %in% x))
search_references(search_string, works_published)
search_references(search_string, works_published)
#
search_publisher("BMJ", works_published)
# Brill : cited ()
publisher_brill <- works_cited_source_issn[grepl("brill", works_cited_source_issn$host_organization_name, ignore.case = TRUE), ]
# Brill : cited ()
publisher_brill <- works_cited_source_issn[grepl("brill", works_cited_source_issn$host_organization, ignore.case = TRUE), ]
##########################################
############# Search Functions #######################
### Search if a publisher is in a DF
# Output the publisher
# @ return: the indices of the publisher
search_publisher <- function(publisher_string, df) {
# Find indices where the host_organization contains the publisher string (case insensitive)
indices_with_string <- which(grepl(publisher_string, df$host_organization, ignore.case = TRUE))
print(df[indices_with_string, ]$host_organization)
print(df[indices_with_string, ]$id)
return(indices_with_string)
}
#### Function: search_work_publisher():
## Search a work's publisher and output the publisher
# @return: index of the DF
search_work_publisher <- function(search_string, df) {
# Find indices where the host_organization contains the search string (case insensitive)
indices_with_string <- which(sapply(df$id, function(x) !is.na(x) && search_string %in% x))
print(df[indices_with_string, ]$host_organization)
print(indices_with_string)
return(indices_with_string)
}
###############################################################
# Verify any cited work using the function search_references()
# Define the function to search for a string in the referenced_works column and print the output
##############################################3
search_references <- function(search_string, df) {
indices_with_string <- which(sapply(df$referenced_works, function(x) search_string %in% x))
print(indices_with_string)
print(df[indices_with_string, ]$id)
}
truncate_and_write <- function(data, file_path_prefix = "citations/") {
data_name <- deparse(substitute(data))
file_path <- paste0(file_path_prefix, data_name, ".xlsx")
truncated_data <- data %>%
mutate(across(where(is.character), ~ ifelse(nchar(.) > 32767, substr(., 1, 32767), .)))
write_xlsx(truncated_data, file_path)
}
truncate_and_write(works_cited_source_issn_brill)
# 2025-02: Brill (https://openalex.org/publishers/p4310320561)
# 2023:
works_cited_source_issn_brill  <- works_cited_source_issn[grepl("Brill", works_cited_source_issn$host_organization, ignore.case = TRUE), ]
Q
Q
Q
q
Q
Q
Q
Q
Q
Q
Q
Q
Q
Q
# 2025-02: Brill (https://openalex.org/publishers/p4310320561)
# 2023:
works_cited_source_issn_brill  <- works_cited_source_issn[grepl("Brill", works_cited_source_issn$host_organization, ignore.case = TRUE), ]
works_cited_source_nonissn_brill <- works_cited_source_nonissn[grepl("Brill", works_cited_source_nonissn$host_organization, ignore.case = TRUE), ]
truncate_and_write(works_cited_source_issn_brill)
works_cited_source_nonissn_brill <- works_cited_source_nonissn[grepl("Brill", works_cited_source_nonissn$host_organization, ignore.case = TRUE), ]
truncate_and_write(works_cited_source_nonissn_brill)
works_published_brill <-search_work_publisher("Brill", works_published)
works_published_brill <- works_published[grepl("Brill", works_published$host_organization, ignore.case = TRUE), ]
truncate_and_write(works_published_brill)
View(works_published_brill)
id_counts <-table(works_cited_source_issn_brill$id)
duplicateds <- id_counts[id_counts >= 1]
print(id_counts)
rank_top_cited_journals <- function(data, journal_col, top_n = 10) {
top_cited_journals <- data %>%
group_by(!!sym(journal_col)) %>%      # Group by the journal names (column provided by the user)
summarise(citation_count = n()) %>%   # Count the number of articles per journal
arrange(desc(citation_count)) %>%     # Sort by citation count in descending order
slice(1:top_n)                        # Select top 'n' journals
print(top_cited_journals, n = top_n)
}
rank_top_cited_journals(works_cited_source_issn_brill, "so")
View(works_cited_source_issn_brill)
