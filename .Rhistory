UA_host <- oa_fetch(
entity = "works",
# UA campus repository ID does not work as a filter
best_oa_location.source.host_organization = "https://openalex.org/I138006243",
# If only need count, uncomment the below line for a quick run.
# count_only = TRUE
# If only need some samples. using the below line.
# options = list(sample = 100, seed = 1)
)
View(UA_host)
# locations:
# count: 14909 (2024-07-11)
UA_host3 <- oa_fetch (
entity = "works",
locations.source.host_organization = "https://openalex.org/I138006243",
#count_only = TRUE
)
############### Use campus repository source ID.
# no result using UA campus repository source ID.
UA_host5 <- oa_fetch (
entity = "works",
best_oa_location.source.host_organization = "https://openalex.org/S2764879211",
count_only = TRUE
)
UA_host6 <- oa_fetch (
entity = "works",
locations.source.host_organization = "https://openalex.org/S2764879211",
count_only = TRUE
)
### Filtering source = journal of rangement management
class (UA_host)
# locations:
# count: 14909 (2024-07-11)
UA_host3 <- oa_fetch (
entity = "works",
locations.source.host_organization = "https://openalex.org/I138006243",
#count_only = TRUE
)
View(UA_host3)
View(UA_host3)
# Filter the dataframe to get all rows where "so" is "journal of range management" (JRM) (case insenstive)
df_jrm <- subset(UA_host3, grepl("journal of range management", so, ignore.case = TRUE))
# 3,991 from JRM.
count_jrm <- nrow(df_jrm)
# Filter the dataframe to get all rows where "so" is "journal of range management" (JRM) (case insenstive)
df_jrm <- subset(UA_host3, grepl("journal of range management", so, ignore.case = TRUE))
# 3,991 from JRM.
count_jrm <- nrow(df_jrm)
# Count the number of occurrences of each unique value in the "source" column using dplyr
source_counts_df <- UA_host %>%
count(so, sort = TRUE)
# Display the dataframe with counts
print(source_counts_df)
# Count the number of occurrences of each unique value in the "source" column using dplyr
source_counts_df <- UA_host3 %>%
count(so, sort = TRUE)
# Display the dataframe with counts
# JRM/REM:  3991 + 804
# Rangelands: 685
# NA: 384
print(source_counts_df)
print(select(source_counts_df, 'id', 'so'))
(select(source_counts_df, 'id', 'so'))
library(dplyr)
(select(source_counts_df, 'id', 'so'))
View(source_counts_df)
(select(UA_host3, 'id', 'so'))
## counting DOIs
# Count rows with DOIs
count_with_doi <- sum(grepl("^10\\.", UA_hosts$URL))
## counting DOIs
# Count rows with DOIs
count_with_doi <- sum(grepl("^10\\.", UA_host3$URL))
## counting DOIs
# Count rows with DOIs
count_with_doi <- sum(grepl("^10\\.", UA_host3$url))
## counting DOIs
# Count rows with DOIs
count_with_doi <- df %>% filter(grepl("doi.org", URL, ignore.case = TRUE)) %>% nrow()
## counting DOIs
# Count rows with DOIs
count_with_doi <- UA_host3 %>% filter(grepl("doi.org", url, ignore.case = TRUE)) %>% nrow()
# Count rows without DOIs
count_without_doi <- nrow(df) - count_with_doi
# Print the counts
print(paste("Count of rows with DOI:", count_with_doi))
print(paste("Count of rows without DOI:", count_without_doi))
# Count the number of occurrences of each unique value in the "source" column using dplyr
source_counts_df <- UA_host3 %>%
count(so, sort = TRUE)
print(sourc
# Display the dataframe with counts
# JRM/REM:  4183+836 (3991 + 804 for best_oa)
# Rangelands: 782 (685 for best_oa)
# NA: 384
print(source_counts_df)
# Display the dataframe with counts
# JRM/REM:  4183+836 (3991 + 804 for best_oa)
# Rangelands: 782 (685 for best_oa)
# NA: 384
print(source_counts_df)
count2 <- UA_host3 %>% filter(grepl("10", url, ignore.case = TRUE)) %>% nrow()
count2 <- UA_host3 %>% filter(grepl("10.2458", url, ignore.case = TRUE)) %>% nrow()
ua_doi1 <- UA_host3 %>% filter(grepl("10.2458", url, ignore.case = TRUE))
View(ua_doi1)
rem_doi <- UA_host %>% filter(grepl("10.1016", url, ignore.case=TRUE))
View(rem_doi)
# All locations:
# count: 14903 (2024-07-11)
UA_host_all_location <- oa_fetch (
entity = "works",
locations.source.host_organization = "https://openalex.org/I138006243",
#count_only = TRUE
)
# Best OA location. find out host organization.
# count: 8394 (2024-07-11)
# This fetch will take a few minutes. So be patient .
UA_host_best_location <- oa_fetch(
entity = "works",
# UA campus repository ID does not work as a filter
best_oa_location.source.host_organization = "https://openalex.org/I138006243",
# If only need count, uncomment the below line for a quick run.
count_only = TRUE
# If only need some samples. using the below line.
# options = list(sample = 100, seed = 1)
)
df_rem <- subset(UA_host_all_location, grepl("Range Ecology and Management", so, ignore.case = TRUE))
# 4,183 from JRM.
count_jrm <- nrow(df_jrm) + nrow (df_rem)
# Filter the dataframe to get all rows where "so" is "journal of range management" (JRM) (case insenstive)
df_jrm <- subset(UA_host_all_location, grepl("journal of range management", so, ignore.case = TRUE))
df_rem <- subset(UA_host_all_location, grepl("Range Ecology and Management", so, ignore.case = TRUE))
#
count_jrm <- nrow(df_jrm) + nrow (df_rem)
df_rem <- subset(UA_host_all_location, grepl("Range Ecology and Management", so, ignore.case = TRUE)) # 432
#
count_jrm <- nrow(df_jrm) + nrow (df_rem)
# Count the number of occurrences of each unique value in the "source" column using dplyr
source_counts_df <- UA_host_all_location %>%
count(so, sort = TRUE)
# Display the dataframe with counts
# JRM/REM:  4183+836 (3991 + 804 for best_oa)
# Rangelands: 782 (685 for best_oa)
# NA: 384
print(source_counts_df)
### get metadata for a DOI
install.package("rcorssref")
### get metadata for a DOI
install.packages("rcorssref")
library(rcrossref)
### get metadata for a DOI
install.packages("rcrossref")
install.packages("rcrossref")
library(rcrossref)
# Function to get metadata for a DOI
get_doi_metadata <- function(doi) {
metadata <- cr_cn(doi, format = "citeproc-json")
return(metadata)
}
doi <- "https://doi.org/10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
### get metadata for a DOI
install.packages("rcrossref")
library(rcrossref)
# Function to get metadata for a DOI
get_doi_metadata <- function(doi) {
metadata <- cr_cn(doi, format = "citeproc-json")
return(metadata)
}
doi <- "https://doi.org/10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
# Function to get metadata for a DOI
get_doi_metadata <- function(doi) {
tryCatch({
metadata <- cr_cn(doi, format = "citeproc-json")
return(metadata)
}, error = function(e) {
message("Error retrieving metadata: ", e)
return(NULL)
})
}
doi <- "https://doi.org/10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
# Print the metadata
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
library (httr)
library(jsonlite)
# Function to get metadata for a DOI
get_doi_metadata <- function(doi) {
url <- paste0("https://api.crossref.org/works/", doi)
response <- GET(url)
if (status_code(response) == 200) {
metadata <- content(response, as = "text", encoding = "UTF-8")
metadata <- fromJSON(metadata, flatten = TRUE)
return(metadata$message)
} else {
message("Error retrieving metadata: ", status_code(response))
return(NULL)
}
}
# Example DOI
doi <- "10.2458/v24i1.22003"
# Get metadata for the example DOI
metadata <- get_doi_metadata(doi)
# Print the metadata
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
### Example 2:
doi <- "https://doi.org/10.2458/azu_jrm_v57i2_cox"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
# code generated by GPT-4
# check "is_oa_anywhere" field
# Set the filter for works that are open access anywhere
oa_filter <- "is_oa_anywhere:true"
# Search for works with the specified filter
results <- oa_search(
entity = "works",
filter = oa_filter,
per_page = 100  # Adjust per_page as needed
)
### Example 2:
doi <- "https://doi.org/10.2458/azu_jrm_v57i2_cox"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
View(metadata)
df_rem <- subset(UA_host_all_location, grepl("Range Ecology", so, ignore.case = TRUE)) # 432
# Filter the dataframe to get all rows where "so" is "journal of range management" (JRM) (case insenstive)
df_jrm <- subset(UA_host_all_location, grepl("journal of range management", so, ignore.case = TRUE))  #4,183
df_rem <- subset(UA_host_all_location, grepl("Range Ecology", so, ignore.case = TRUE)) # 432
# Count the number of occurrences of each unique value in the "source" column using dplyr
source_counts_df <- UA_host_all_location %>%
count(so, sort = TRUE)
# common libraries
library(openalexR)
packageVersion("openalexR")
library(dplyr)
library(ggplot2)
library(knitr)
library(testthat)
library(readr)
citation("openalexR")
# check to see if openAlexR has the latest entities in OpenAlex (OpenAlex updated its data model(Entities) in June 2023)
# Before April 2023: they are [1] "works"        "authors"      "venues"       "institutions" "concepts"
# If not, need to use openalexR developer's version
oa_entities()
options (openalexR.mailto="yhan@arizona.edu")
getwd()
setwd("/home/yhan/Documents/openalexR-test")
### Test data
test_data_UAL_authors     <- c("Yan Han", "Ellen Dubinski", "Fernando Rios", "Ahlam Saleh")
test_data_COM_authors     <- c("Phillip Kuo", "Bekir Tanriover", "Ahlam Saleh")
test_data_COM_Tucson_authors <- c("Che Carrie Liu", "Robert M. Aaronson", "Alexa Aasronson", "Mohammed Abbas", "")
test_data_science_authors <- c("Marek Rychlik", "Ali Bilgin", "Beichuan Zhang")
test_data_ischool_authors <- c("Hong Cui")
test_data_others          <- c("Leila Hudson", "Mona Hymel")
test_data_not_updated_authors <-c("Karen Padilla", "Haw-chih Tai")
test_data_affiliation <- c("University of Arizona")
test_data_year <- c("2022", "2021", "2020", "2012")
# Test works
works_from_dois <- oa_fetch(entity = "works", doi = c("https://doi.org/10.1093/ofid/ofac186", "https://doi.org/10.1007/s11192-013-1221-3"),  verbose = TRUE)
### Testing three datasets citations recall and precision using one article (published in 2022)
### OpenAlex: Precision
### OpenCitaitons: Precision 100%. Recall: 2/3
### Google scholar: Precision 100%. Recall 100%
works_from_dois <- oa_fetch(entity = "works", doi = c("https://doi.org/10.6017/ital.v40i1.12553"),  verbose = TRUE)
works_from_dois$cited_by_api_url
works_from_dois$ids
# All locations:
# count: 14903 (2024-07-11)
UA_host_all_location <- oa_fetch (
entity = "works",
locations.source.host_organization = "https://openalex.org/I138006243",
#count_only = TRUE
)
View(source_counts_df)
# common libraries
library(openalexR)
packageVersion("openalexR")
library(dplyr)
library(ggplot2)
library(knitr)
library(testthat)
library(readr)
citation("openalexR")
# check to see if openAlexR has the latest entities in OpenAlex (OpenAlex updated its data model(Entities) in June 2023)
# Before April 2023: they are [1] "works"        "authors"      "venues"       "institutions" "concepts"
# If not, need to use openalexR developer's version
oa_entities()
options (openalexR.mailto="yhan@arizona.edu")
getwd()
setwd("/home/yhan/Documents/openalexR-test")
### Test data
test_data_UAL_authors     <- c("Yan Han", "Ellen Dubinski", "Fernando Rios", "Ahlam Saleh")
test_data_COM_authors     <- c("Phillip Kuo", "Bekir Tanriover", "Ahlam Saleh")
test_data_COM_Tucson_authors <- c("Che Carrie Liu", "Robert M. Aaronson", "Alexa Aasronson", "Mohammed Abbas", "")
test_data_science_authors <- c("Marek Rychlik", "Ali Bilgin", "Beichuan Zhang")
test_data_ischool_authors <- c("Hong Cui")
test_data_others          <- c("Leila Hudson", "Mona Hymel")
test_data_not_updated_authors <-c("Karen Padilla", "Haw-chih Tai")
test_data_affiliation <- c("University of Arizona")
test_data_year <- c("2022", "2021", "2020", "2012")
# Best OA location. find out host organization.
# count: 8394 (2024-07-11)
# This fetch will take a few minutes. So be patient .
UA_host_best_location <- oa_fetch(
entity = "works",
# UA campus repository ID does not work as a filter
best_oa_location.source.host_organization = "https://openalex.org/I138006243",
# If only need count, uncomment the below line for a quick run.
count_only = TRUE
# If only need some samples. using the below line.
# options = list(sample = 100, seed = 1)
)
# Primary_location.source.host_organization.
# count: 24 (2024-07-11)
UA_host2 <- oa_fetch (
entity = "works",
primary_location.source.host_organization = "https://openalex.org/I138006243",
count_only = TRUE
)
############### Use campus repository source ID.
# no result using UA campus repository source ID.
UA_host5 <- oa_fetch (
entity = "works",
best_oa_location.source.host_organization = "https://openalex.org/S2764879211",
count_only = TRUE
)
UA_host6 <- oa_fetch (
entity = "works",
locations.source.host_organization = "https://openalex.org/S2764879211",
count_only = TRUE
)
# Filter the dataframe to get all rows where "so" is "journal of range management" (JRM) (case insenstive)
df_jrm <- subset(UA_host_all_location, grepl("journal of range management", so, ignore.case = TRUE))  #4,183
df_rem <- subset(UA_host_all_location, grepl("Range Ecology", so, ignore.case = TRUE)) # 432
# Count the number of occurrences of each unique value in the "source" column using dplyr
source_counts_df <- UA_host_all_location %>%
count(so, sort = TRUE)
# Display the dataframe with counts
# JRM/REM:  4183+836 (3991 + 804 for best_oa)
# Rangelands: 782 (685 for best_oa)
# NA: 384
print(source_counts_df)
#### Example 1: More like openalex pulled directly from crossref.
###  Campus repo:
doi <- "10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
library(rcrossref)
library (httr)
library(jsonlite)
# Function to get metadata for a DOI
get_doi_metadata <- function(doi) {
url <- paste0("https://api.crossref.org/works/", doi)
response <- GET(url)
if (status_code(response) == 200) {
metadata <- content(response, as = "text", encoding = "UTF-8")
metadata <- fromJSON(metadata, flatten = TRUE)
return(metadata$message)
} else {
message("Error retrieving metadata: ", status_code(response))
return(NULL)
}
}
#### Example 1: More like openalex pulled directly from crossref.
###  Campus repo:
doi <- "10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
### Example 3:
# https://doi.org/10.1038/ng.3667"
doi <- "https://doi.org/10.1038/ng.3667"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
ls
### 1.2 Getting all the works based on the institution ROR and publication date. It takes longer time.
# see above for the running time
org_works_2019 <-oa_fetch(
entity="works",
institutions.ror=c("03m2x1q45"),
from_publication_date ="2019-01-01",
to_publication_date = "2019-12-31"
)
library(openalexR)
library(dplyr)
library(tidyverse)
library(writexl)
# free unused obj to manage memory
gc()
rm(list=ls())
gc()
options("max.print" = 100000)
options (openalexR.mailto="yhan@arizona.edu")
getwd()
setwd("/home/yhan/Documents/openalexR-test/")
### 1.1 Getting the count only. This is the quick way to find out the total number of works.
# Typically only some seconds
UAworks_count <-oa_fetch(
entity="works",
institutions.ror=c("03m2x1q45"),
from_publication_date ="2021-01-01",
to_publication_date = "2021-12-31",
count_only = TRUE
)
### 1.1 Getting the count only. This is the quick way to find out the total number of works.
# Typically only some seconds
UAworks_count <-oa_fetch(
entity="works",
institutions.ror=c("03m2x1q45"),
from_publication_date ="2022-01-01",
to_publication_date = "2022-12-31",
count_only = TRUE
)
org_works_2022 <- readRDS("../org_works_2022.rds")
org_works <- org_works_2022
##### 2. Checking and verifying data
##### 2.1 Route 1: Getting citation data from $referenced_works
##### Route 2: Getting author's data?
###### change this line only to update the right dataset.
org_works_ref <- org_works$referenced_works
# Filter the rows where $reference_works is NA and $type is "article"
works_na_referenced_works <- org_works %>%
filter(is.na(referenced_works) & type == "article")
write_xlsx(works_na_referenced_works, "citations/works_2022_na_referenced_works.xlsx")
# this na_indices include type: article, books, errata, letter, and other types
na_indices <- which(sapply(org_works_ref, function(x) is.logical(x) && is.na(x)))
na_count <- sum(sapply(org_works_ref, function(x) is.logical(x) && is.na(x)))
na_percent <- na_count/length(org_works_ref) * 100
# Remove NA, logical(0) from list (Meaning: no references)
org_works_ref <- Filter(function(x) length(x) > 0, org_works_ref)
class(org_works_ref)
org_works_ref_combined <- unlist(org_works_ref, use.names = FALSE)
org_works_ref_combined <- org_works_ref_combined[!is.na(org_works_ref_combined)]  # Remove NA values
### 2.21 finding these duplicates, which mean the duplicates have been cited multiple times
# (probably more important to have these journals subscribed!)
# cited more: ~20% - 25%  (2022, 2023 UArizona data)
org_works_ref_more_cited <- org_works_ref_combined[duplicated(org_works_ref_combined)]
#Creating an empty dataframe to store the results of the for loop.
works_cited <-data.frame()
### 3.33 Testing if a cited work is found.
# Deep Learning, Nature, by Yann LeCun, Yoshua Bengio, Geoffrey Hinton. Cited by: 62,210
search_string <- "https://openalex.org/W2919115771"
result <- lapply(org_works_ref_combined, function(x) grep(search_string, x, value = TRUE))
print(result)
matches <- result[sapply(result, length) > 0]
print(matches)
indices <- which(sapply(org_works_ref_combined, function(x) any(grepl(search_string, x))))
for (i in indices) {
cat("Index:", i, "\n")
cat("Element:\n", org_works_ref_combined[[i]], "\n\n")
}
# Find it from the original article
search_string <- "https://openalex.org/W2594545996"
# this article was cited 81 times in 2019, cited 130 times in 2020, cited 16 times in 2023,
indices_with_string <- which(sapply(org_works$referenced_works, function(x) search_string %in% x))
print(indices_with_string)
org_works[indices_with_string, ]$id
# test case 2: cited 6 from microbiology
# both final published version and pre-print existing: https://openalex.org/works/W4379795917 and https://openalex.org/W4319339791
search_string <- "https://openalex.org/W2128159409"
indices_with_string <- which(sapply(org_works$referenced_works, function(x) search_string %in% x))
print(indices_with_string)
org_works[indices_with_string, ]$id
works_cited_final <- readRDS("../works_cited_final_2022.rds")
works_cited_final <- readRDS("../works_cited_2022.rds")
works_cited_final <- readRDS("../works_cited_2022_1.rds")
works_cited_final <- readRDS("../works_cited_2022-1.rds")
articles_cited <- works_cited_final[!(is.na(works_cited_final$issn_l)), ]
articles_cited <- articles_cited[!(is.na(articles_cited$issn_l) | articles_cited$issn_l == ""), ]
nrow(articles_cited)
org_works_2022 <-oa_fetch(
entity="works",
institutions.ror=c("03m2x1q45"),
from_publication_date ="2022-01-01",
to_publication_date = "2022-12-31"
)
# saveRDS(org_works_2019, "../org_works_2019.rds")
# saveRDS(org_works_2020, "../org_works_2020.rds")
# saveRDS(org_works_2021, "../org_works_2021.rds")
saveRDS(org_works_2022, "../org_works_2022.rds")
