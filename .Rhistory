### Example 2:
doi <- "https://doi.org/10.2458/azu_jrm_v57i2_cox"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
View(metadata)
df_rem <- subset(UA_host_all_location, grepl("Range Ecology", so, ignore.case = TRUE)) # 432
# Filter the dataframe to get all rows where "so" is "journal of range management" (JRM) (case insenstive)
df_jrm <- subset(UA_host_all_location, grepl("journal of range management", so, ignore.case = TRUE))  #4,183
df_rem <- subset(UA_host_all_location, grepl("Range Ecology", so, ignore.case = TRUE)) # 432
# Count the number of occurrences of each unique value in the "source" column using dplyr
source_counts_df <- UA_host_all_location %>%
count(so, sort = TRUE)
# common libraries
library(openalexR)
packageVersion("openalexR")
library(dplyr)
library(ggplot2)
library(knitr)
library(testthat)
library(readr)
citation("openalexR")
# check to see if openAlexR has the latest entities in OpenAlex (OpenAlex updated its data model(Entities) in June 2023)
# Before April 2023: they are [1] "works"        "authors"      "venues"       "institutions" "concepts"
# If not, need to use openalexR developer's version
oa_entities()
options (openalexR.mailto="yhan@arizona.edu")
getwd()
setwd("/home/yhan/Documents/openalexR-test")
### Test data
test_data_UAL_authors     <- c("Yan Han", "Ellen Dubinski", "Fernando Rios", "Ahlam Saleh")
test_data_COM_authors     <- c("Phillip Kuo", "Bekir Tanriover", "Ahlam Saleh")
test_data_COM_Tucson_authors <- c("Che Carrie Liu", "Robert M. Aaronson", "Alexa Aasronson", "Mohammed Abbas", "")
test_data_science_authors <- c("Marek Rychlik", "Ali Bilgin", "Beichuan Zhang")
test_data_ischool_authors <- c("Hong Cui")
test_data_others          <- c("Leila Hudson", "Mona Hymel")
test_data_not_updated_authors <-c("Karen Padilla", "Haw-chih Tai")
test_data_affiliation <- c("University of Arizona")
test_data_year <- c("2022", "2021", "2020", "2012")
# Test works
works_from_dois <- oa_fetch(entity = "works", doi = c("https://doi.org/10.1093/ofid/ofac186", "https://doi.org/10.1007/s11192-013-1221-3"),  verbose = TRUE)
### Testing three datasets citations recall and precision using one article (published in 2022)
### OpenAlex: Precision
### OpenCitaitons: Precision 100%. Recall: 2/3
### Google scholar: Precision 100%. Recall 100%
works_from_dois <- oa_fetch(entity = "works", doi = c("https://doi.org/10.6017/ital.v40i1.12553"),  verbose = TRUE)
works_from_dois$cited_by_api_url
works_from_dois$ids
# All locations:
# count: 14903 (2024-07-11)
UA_host_all_location <- oa_fetch (
entity = "works",
locations.source.host_organization = "https://openalex.org/I138006243",
#count_only = TRUE
)
View(source_counts_df)
# common libraries
library(openalexR)
packageVersion("openalexR")
library(dplyr)
library(ggplot2)
library(knitr)
library(testthat)
library(readr)
citation("openalexR")
# check to see if openAlexR has the latest entities in OpenAlex (OpenAlex updated its data model(Entities) in June 2023)
# Before April 2023: they are [1] "works"        "authors"      "venues"       "institutions" "concepts"
# If not, need to use openalexR developer's version
oa_entities()
options (openalexR.mailto="yhan@arizona.edu")
getwd()
setwd("/home/yhan/Documents/openalexR-test")
### Test data
test_data_UAL_authors     <- c("Yan Han", "Ellen Dubinski", "Fernando Rios", "Ahlam Saleh")
test_data_COM_authors     <- c("Phillip Kuo", "Bekir Tanriover", "Ahlam Saleh")
test_data_COM_Tucson_authors <- c("Che Carrie Liu", "Robert M. Aaronson", "Alexa Aasronson", "Mohammed Abbas", "")
test_data_science_authors <- c("Marek Rychlik", "Ali Bilgin", "Beichuan Zhang")
test_data_ischool_authors <- c("Hong Cui")
test_data_others          <- c("Leila Hudson", "Mona Hymel")
test_data_not_updated_authors <-c("Karen Padilla", "Haw-chih Tai")
test_data_affiliation <- c("University of Arizona")
test_data_year <- c("2022", "2021", "2020", "2012")
# Best OA location. find out host organization.
# count: 8394 (2024-07-11)
# This fetch will take a few minutes. So be patient .
UA_host_best_location <- oa_fetch(
entity = "works",
# UA campus repository ID does not work as a filter
best_oa_location.source.host_organization = "https://openalex.org/I138006243",
# If only need count, uncomment the below line for a quick run.
count_only = TRUE
# If only need some samples. using the below line.
# options = list(sample = 100, seed = 1)
)
# Primary_location.source.host_organization.
# count: 24 (2024-07-11)
UA_host2 <- oa_fetch (
entity = "works",
primary_location.source.host_organization = "https://openalex.org/I138006243",
count_only = TRUE
)
############### Use campus repository source ID.
# no result using UA campus repository source ID.
UA_host5 <- oa_fetch (
entity = "works",
best_oa_location.source.host_organization = "https://openalex.org/S2764879211",
count_only = TRUE
)
UA_host6 <- oa_fetch (
entity = "works",
locations.source.host_organization = "https://openalex.org/S2764879211",
count_only = TRUE
)
# Filter the dataframe to get all rows where "so" is "journal of range management" (JRM) (case insenstive)
df_jrm <- subset(UA_host_all_location, grepl("journal of range management", so, ignore.case = TRUE))  #4,183
df_rem <- subset(UA_host_all_location, grepl("Range Ecology", so, ignore.case = TRUE)) # 432
# Count the number of occurrences of each unique value in the "source" column using dplyr
source_counts_df <- UA_host_all_location %>%
count(so, sort = TRUE)
# Display the dataframe with counts
# JRM/REM:  4183+836 (3991 + 804 for best_oa)
# Rangelands: 782 (685 for best_oa)
# NA: 384
print(source_counts_df)
#### Example 1: More like openalex pulled directly from crossref.
###  Campus repo:
doi <- "10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
library(rcrossref)
library (httr)
library(jsonlite)
# Function to get metadata for a DOI
get_doi_metadata <- function(doi) {
url <- paste0("https://api.crossref.org/works/", doi)
response <- GET(url)
if (status_code(response) == 200) {
metadata <- content(response, as = "text", encoding = "UTF-8")
metadata <- fromJSON(metadata, flatten = TRUE)
return(metadata$message)
} else {
message("Error retrieving metadata: ", status_code(response))
return(NULL)
}
}
#### Example 1: More like openalex pulled directly from crossref.
###  Campus repo:
doi <- "10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
### Example 3:
# https://doi.org/10.1038/ng.3667"
doi <- "https://doi.org/10.1038/ng.3667"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
ls
### 1.2 Getting all the works based on the institution ROR and publication date. It takes longer time.
# see above for the running time
org_works_2019 <-oa_fetch(
entity="works",
institutions.ror=c("03m2x1q45"),
from_publication_date ="2019-01-01",
to_publication_date = "2019-12-31"
)
library(readr)
library(httr)
library(jsonlite)
library(dplyr)
# Step 1: Read the CSV file
file_path <- "2024-10-29.csv"  # Replace with your actual file path
data <- read_csv(file_path)
# Step 2: Extract the "DOI" column and remove empty cells
doi_column <- data$DOI[!is.na(data$DOI) & data$DOI != ""]
get_doi_metadata <- function(doi) {
# Format the URL for the CrossRef API request
url <- paste0("https://api.crossref.org/works/", doi)
# Make the GET request
response <- GET(url)
# Check if the request was successful
if (status_code(response) == 200) {
# Parse the JSON content
data <- fromJSON(content(response, "text", encoding = "UTF-8"))
# Return the metadata part of the response
return(data$message)
} else {
warning(paste("Failed to fetch metadata for DOI:", doi))
return(NULL)
}
}
# Example list of DOIs (replace this with your DOI vector from the CSV)
doi_column <- c("10.1007/s11069-020-03875-3", "10.1111/ssqu.12699")
# Create an empty list to store metadata
metadata_list <- lapply(doi_column, get_doi_metadata)
str(metadata_list[[1]])  # Check the structure of the first element
# Convert metadata_list into a structured data frame
metadata_df <- bind_rows(lapply(metadata_list, function(x) {
if (is.list(x) && !is.null(x)) {
tryCatch({
data.frame(
doi = ifelse(!is.null(x$DOI), x$DOI, NA),
title = ifelse(!is.null(x$title), x$title, NA),
abstract = ifelse(!is.null(x$abstract), x$abstract, NA),  # Corrected abstract field
author = ifelse(!is.null(x$author),
paste(x$author$family, collapse = ", "),
NA),
published_date = ifelse(!is.null(x$`published-print`$`date-parts`),
paste(x$`published-print`$`date-parts`[[1]], collapse = "-"),
NA),
journal = ifelse(!is.null(x$`short-container-title`), x$`short-container-title`, NA),
stringsAsFactors = FALSE
)
}, error = function(e) {
data.frame(
doi = NA, title = NA, abstract = NA, author = NA, published_date = NA, journal = NA
)
})
} else {
NULL
}
}))
# Print the resulting data frame
print(metadata_df)
# Step 2: Extract the "DOI" column and remove empty cells
doi_column <- data$DOI[!is.na(data$DOI) & data$DOI != ""]
# Step 1: Read the CSV file
file_path <- "2024-10-29.csv"  # Replace with your actual file path
data <- read_csv(file_path)
# Step 1: Read the CSV file
getwd()
setwd("/home/yhan/Documents/openalexR-test/")
file_path <- "2024-10-29.csv"  # Replace with your actual file path
data <- read_csv(file_path)
# Step 2: Extract the "DOI" column and remove empty cells
doi_column <- data$DOI[!is.na(data$DOI) & data$DOI != ""]
get_doi_metadata <- function(doi) {
# Format the URL for the CrossRef API request
url <- paste0("https://api.crossref.org/works/", doi)
response <- GET(url)
# Check if the request was successful
if (status_code(response) == 200) {
# Parse the JSON content
data <- fromJSON(content(response, "text", encoding = "UTF-8"))
# Return the metadata part of the response
return(data$message)
} else {
warning(paste("Failed to fetch metadata for DOI:", doi))
return(NULL)
}
}
# Create an empty list to store metadata
metadata_list <- lapply(doi_column, get_doi_metadata)
str(metadata_list[[1]])  # Check the structure of the first element
library(readr)
library(httr)
library(jsonlite)
library(dplyr)
# Step 1: Read the CSV file
getwd()
setwd("/home/yhan/Documents/openalexR-test/")
file_path <- "2024-10-29.csv"  # Replace with your actual file path
data <- read_csv(file_path)
# Step 2: Extract the "DOI" column and remove empty cells
doi_column <- data$DOI[!is.na(data$DOI) & data$DOI != ""]
class(doi_column)
# Example list of DOIs (replace this with your DOI vector from the CSV)
#doi_column <- c("10.1007/s11069-020-03875-3", "10.1111/ssqu.12699")
doi_sample <- doi_column[1:10]
# Example list of DOIs (replace this with your DOI vector from the CSV)
#doi_column <- c("10.1007/s11069-020-03875-3", "10.1111/ssqu.12699")
doi_sample <- doi_column[1:2]
# Create an empty list to store metadata
metadata_list <- lapply(doi_sample, get_doi_metadata)
get_doi_metadata <- function(doi) {
# Format the URL for the CrossRef API request
url <- paste0("https://api.crossref.org/works/", doi)
response <- GET(url)
# Check if the request was successful
if (status_code(response) == 200) {
# Parse the JSON content
data <- fromJSON(content(response, "text", encoding = "UTF-8"))
# Return the metadata part of the response
return(data$message)
} else {
warning(paste("Failed to fetch metadata for DOI:", doi))
return(NULL)
}
}
# Example list of DOIs (replace this with your DOI vector from the CSV)
#doi_column <- c("10.1007/s11069-020-03875-3", "10.1111/ssqu.12699")
doi_sample <- doi_column[1:2]
# Create an empty list to store metadata
metadata_list <- lapply(doi_sample, get_doi_metadata)
get_doi_metadata <- function(doi) {
# Format the URL for the CrossRef API request
url <- paste0("https://api.crossref.org/works/", doi)
# Retry mechanism: attempt up to 3 times if request fails
attempt <- 1
max_attempts <- 3
while (attempt <= max_attempts) {
response <- tryCatch({
GET(url)
}, error = function(e) {
message("Error fetching DOI:", doi, " - ", e)
return(NULL)
})
# Check if the request was successful
if (!is.null(response) && status_code(response) == 200) {
# Parse the JSON content
data <- fromJSON(content(response, "text", encoding = "UTF-8"))
# Return the metadata part of the response
return(data$message)
} else {
# Log and retry if necessary
warning(paste("Attempt", attempt, "failed for DOI:", doi))
attempt <- attempt + 1
Sys.sleep(1)  # Add a small delay before retrying
}
}
# If all attempts fail, return NULL
message("Failed to fetch metadata after ", max_attempts, " attempts for DOI:", doi)
return(NULL)
}
# Test with the first 10 DOIs, introducing a delay between requests
doi_sample <- doi_column[1:10]
get_doi_metadata <- function(doi) {
# Format the URL for the CrossRef API request
url <- paste0("https://api.crossref.org/works/", doi)
# Retry mechanism: attempt up to 3 times if request fails
attempt <- 1
max_attempts <- 3
while (attempt <= max_attempts) {
response <- tryCatch({
GET(url)
}, error = function(e) {
message("Error fetching DOI:", doi, " - ", e)
return(NULL)
})
# Check if the request was successful
if (!is.null(response) && status_code(response) == 200) {
# Parse the JSON content
data <- fromJSON(content(response, "text", encoding = "UTF-8"))
# Return the metadata part of the response
return(data$message)
} else {
# Log and retry if necessary
warning(paste("Attempt", attempt, "failed for DOI:", doi))
attempt <- attempt + 1
Sys.sleep(1)  # Add a small delay before retrying
}
}
# If all attempts fail, return NULL
message("Failed to fetch metadata after ", max_attempts, " attempts for DOI:", doi)
return(NULL)
}
# Example list of DOIs (replace this with your DOI vector from the CSV)
doi_column <- c("10.1007/s11069-020-03875-3", "10.1111/ssqu.12699")
# Create an empty list to store metadata
metadata_list <- lapply(doi_column, get_doi_metadata)
# Load necessary package
# install.packages("readr")
# install.packages("httr")
# install.packages("jsonlite")
library(readr)
library(httr)
library(jsonlite)
library(dplyr)
# Step 1: Read the CSV file
file_path <- "2024-10-29.csv"  # Replace with your actual file path
data <- read_csv(file_path)
# Step 2: Extract the "DOI" column and remove empty cells
doi_column <- data$DOI[!is.na(data$DOI) & data$DOI != ""]
get_doi_metadata <- function(doi) {
# Format the URL for the CrossRef API request
url <- paste0("https://api.crossref.org/works/", doi)
# Retry mechanism: attempt up to 3 times if request fails
attempt <- 1
max_attempts <- 3
while (attempt <= max_attempts) {
response <- tryCatch({
GET(url)
}, error = function(e) {
message("Error fetching DOI:", doi, " - ", e)
return(NULL)
})
# Check if the request was successful
if (!is.null(response) && status_code(response) == 200) {
# Parse the JSON content
data <- fromJSON(content(response, "text", encoding = "UTF-8"))
# Return the metadata part of the response
return(data$message)
} else {
# Log and retry if necessary
warning(paste("Attempt", attempt, "failed for DOI:", doi))
attempt <- attempt + 1
Sys.sleep(1)  # Add a small delay before retrying
}
}
# If all attempts fail, return NULL
message("Failed to fetch metadata after ", max_attempts, " attempts for DOI:", doi)
return(NULL)
}
# Example list of DOIs (replace this with your DOI vector from the CSV)
doi_column <- c("10.1007/s11069-020-03875-3", "10.1111/ssqu.12699")
# Create an empty list to store metadata
metadata_list <- lapply(doi_column, get_doi_metadata)
get_doi_metadata <- function(doi) {
# Construct the CrossRef API URL for the DOI
url <- paste0("https://api.crossref.org/works/", doi)
# Make the GET request
response <- GET(url)
# Check if the request was successful
if (status_code(response) == 200) {
# Parse the JSON content
data <- fromJSON(content(response, "text", encoding = "UTF-8"))
# Return the metadata part of the response
return(data$message)
} else {
warning(paste("Failed to fetch metadata for DOI:", doi))
return(NULL)
}
}
# Create an empty list to store metadata
metadata_list <- lapply(doi_column, get_doi_metadata)
# Example list of DOIs (replace this with your DOI vector from the CSV)
doi_column <- c("10.1007/s11069-020-03875-3", "10.1111/ssqu.12699")
get_doi_metadata <- function(doi) {
# Construct the CrossRef API URL for the DOI
url <- paste0("https://api.crossref.org/works/", doi)
# Make the GET request
response <- GET(url)
# Check if the request was successful
if (status_code(response) == 200) {
# Parse the JSON content
data <- fromJSON(content(response, "text", encoding = "UTF-8"))
# Return the metadata part of the response
return(data$message)
} else {
warning(paste("Failed to fetch metadata for DOI:", doi))
return(NULL)
}
}
# Create an empty list to store metadata
metadata_list <- lapply(doi_column, get_doi_metadata)
get_doi_metadata <- function(doi) {
# Construct the CrossRef API URL for the DOI
url <- paste0("https://api.crossref.org/works/", doi)
# Make the GET request
response <- GET(url)
# Check if the request was successful
if (status_code(response) == 200) {
# Parse the JSON content
data <- fromJSON(content(response, "text", encoding = "UTF-8"))
# Return the metadata part of the response
return(data$message)
} else {
warning(paste("Failed to fetch metadata for DOI:", doi))
return(NULL)
}
}
# Create an empty list to store metadata
metadata_list <- lapply(doi_column, get_doi_metadata)
# Load necessary package
# install.packages("readr")  # Uncomment if you don't have the readr package
library(readr)
# install.packages("httr")
# install.packages("jsonlite")
library(httr)
library(jsonlite)
library(dplyr)
# Step 1: Read the CSV file
file_path <- "2024-10-29.csv"  # Replace with your actual file path
data <- read_csv(file_path)
# Step 2: Extract the "DOI" column and remove empty cells
doi_column <- data$DOI[!is.na(data$DOI) & data$DOI != ""]
get_doi_metadata <- function(doi) {
# Format the URL for the CrossRef API request
url <- paste0("https://api.crossref.org/works/", doi)
# Make the GET request
response <- GET(url)
# Check if the request was successful
if (status_code(response) == 200) {
# Parse the JSON content
data <- fromJSON(content(response, "text", encoding = "UTF-8"))
# Return the metadata part of the response
return(data$message)
} else {
warning(paste("Failed to fetch metadata for DOI:", doi))
return(NULL)
}
}
# Example list of DOIs (replace this with your DOI vector from the CSV)
doi_column <- c("10.1007/s11069-020-03875-3", "10.1111/ssqu.12699")
# Create an empty list to store metadata
metadata_list <- lapply(doi_column, get_doi_metadata)
