UA_host_best_location <- oa_fetch(
entity = "works",
# UA campus repository ID does not work as a filter
best_oa_location.source.host_organization = "https://openalex.org/I138006243",
# If only need count, uncomment the below line for a quick run.
count_only = TRUE
# If only need some samples. using the below line.
# options = list(sample = 100, seed = 1)
)
# Primary_location.source.host_organization.
# count: 24 (2024-07-11)
UA_host2 <- oa_fetch (
entity = "works",
primary_location.source.host_organization = "https://openalex.org/I138006243",
count_only = TRUE
)
############### Use campus repository source ID.
# no result using UA campus repository source ID.
UA_host5 <- oa_fetch (
entity = "works",
best_oa_location.source.host_organization = "https://openalex.org/S2764879211",
count_only = TRUE
)
UA_host6 <- oa_fetch (
entity = "works",
locations.source.host_organization = "https://openalex.org/S2764879211",
count_only = TRUE
)
# Filter the dataframe to get all rows where "so" is "journal of range management" (JRM) (case insenstive)
df_jrm <- subset(UA_host_all_location, grepl("journal of range management", so, ignore.case = TRUE))  #4,183
df_rem <- subset(UA_host_all_location, grepl("Range Ecology", so, ignore.case = TRUE)) # 432
# Count the number of occurrences of each unique value in the "source" column using dplyr
source_counts_df <- UA_host_all_location %>%
count(so, sort = TRUE)
# Display the dataframe with counts
# JRM/REM:  4183+836 (3991 + 804 for best_oa)
# Rangelands: 782 (685 for best_oa)
# NA: 384
print(source_counts_df)
#### Example 1: More like openalex pulled directly from crossref.
###  Campus repo:
doi <- "10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
library(rcrossref)
library (httr)
library(jsonlite)
# Function to get metadata for a DOI
get_doi_metadata <- function(doi) {
url <- paste0("https://api.crossref.org/works/", doi)
response <- GET(url)
if (status_code(response) == 200) {
metadata <- content(response, as = "text", encoding = "UTF-8")
metadata <- fromJSON(metadata, flatten = TRUE)
return(metadata$message)
} else {
message("Error retrieving metadata: ", status_code(response))
return(NULL)
}
}
#### Example 1: More like openalex pulled directly from crossref.
###  Campus repo:
doi <- "10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
### Example 3:
# https://doi.org/10.1038/ng.3667"
doi <- "https://doi.org/10.1038/ng.3667"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
ls
### 1.2 Getting all the works based on the institution ROR and publication date. It takes longer time.
# see above for the running time
org_works_2019 <-oa_fetch(
entity="works",
institutions.ror=c("03m2x1q45"),
from_publication_date ="2019-01-01",
to_publication_date = "2019-12-31"
)
library(openalexR)
library(dplyr)
library(tidyverse)
library(writexl)
# free unused obj to manage memory
gc()
rm(list=ls())
gc()
options("max.print" = 100000)
options (openalexR.mailto="yhan@arizona.edu")
getwd()
setwd("/home/yhan/Documents/openalexR-test/")
org_works_2023 <-oa_fetch(
entity="works",
institutions.ror=c("03m2x1q45"),
from_publication_date ="2023-01-01",
to_publication_date = "2023-12-31"
)
# saveRDS(org_works_2019, "../org_works_2019.rds")
# saveRDS(org_works_2020, "../org_works_2020.rds")
# saveRDS(org_works_2021, "../org_works_2021.rds")
# saveRDS(org_works_2022, "../org_works_2022.rds")
saveRDS(org_works_2023, "../org_works_2023.rds")
# change working data here
org_works <- org_works_2023
##### 2. Checking and verifying data
##### 2.1 Route 1: Getting citation data from $referenced_works
##### Rout2: Getting author's data?
###### change this line only to update the right dataset.
org_works_ref <- org_works$referenced_works
# Find "NA" indexes: 20-25% no references
# Year 2023: 2273 / 9166 have "NA".
na_indices <- which(sapply(org_works_ref, function(x) is.logical(x) && is.na(x)))
na_count <- sum(sapply(org_works_ref, function(x) is.logical(x) && is.na(x)))
na_percent <- na_count/length(org_works_ref) * 100
# Remove NA, logical(0) from list (Meaning: no references)
org_works_ref <- Filter(function(x) length(x) > 0, org_works_ref)
class(org_works_ref)
org_works_ref_combined <- unlist(org_works_ref, use.names = FALSE)
org_works_ref_combined <- org_works_ref_combined[!is.na(org_works_ref_combined)]  # Remove NA values
##### 3. From authors' DF.
# Flattening authors fields from the DF (multiple authors per work).
# 426,000 obs (multiple authors) from 50,400 obs (works)
org_works_since <- org_works
#Creating an empty dataframe to store the results of the for loop.
works_cited <-data.frame()
### 3.33 Testing if a cited work is found.
# Deep Learning, Nature, by Yann LeCun, Yoshua Bengio, Geoffrey Hinton. Cited by: 62,210
search_string <- "https://openalex.org/W2919115771"
result <- lapply(org_works_ref_combined, function(x) grep(search_string, x, value = TRUE))
print(result)
matches <- result[sapply(result, length) > 0]
print(matches)
indices <- which(sapply(org_works_ref_combined, function(x) any(grepl(search_string, x))))
for (i in indices) {
cat("Index:", i, "\n")
cat("Element:\n", org_works_ref_combined[[i]], "\n\n")
}
# Find it from the original article
search_string <- "https://openalex.org/W2594545996"  # this article was cited 81 times in 2019.
indices_with_string <- which(sapply(org_works$referenced_works, function(x) search_string %in% x))
print(indices_with_string)
org_works[indices_with_string, ]$id
malaria_topic <- oa_fetch(entity = "topics", search = "malaria") %>%
filter(display_name == "Malaria") %>%
pull(id)
malaria_topic
#> [1] "https://openalex.org/T10091"
system.time({
res <- oa_fetch(
topics.id = malaria_topic,
entity = "works",
verbose = TRUE,
options = list(sample = 10000, seed = 1),
output = "list"
)
})
rm(res)
rm(org_works_ref)
fetch_number <- 50
num_of_works <- length (org_works_ref_combined)
time_taken <- system.time({
for(i in seq(1, num_of_works, by=fetch_number)){
batch_identifiers <-org_works_ref_combined[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(identifier=batch_identifiers)
works_cited<-rbind(works_cited, batch_data)
}
})
print(paste("time to run: ", time_taken["elapsed"] / 60, "minutes"))
warnings()
#### Step 1: Re-generate a new row if it matches (meaning; cited multiple times.)
works_cited_final <- works_cited
saveRDS(works_cited_final, "../works_cited_final_2023.rds")
articles_cited <- works_cited_final[!(is.na(works_cited_final$issn_l)), ]
articles_cited <- articles_cited[!(is.na(articles_cited$issn_l) | articles_cited$issn_l == ""), ]
nrow(articles_cited)
# Trim and normalize the host_organization column
articles_cited$host_organization <- trimws(articles_cited$host_organization)
articles_cited$issn_l <- trimws(articles_cited$issn_l)
# Empty or NULL records
count_null_empty_id <- sum(is.na(articles_cited$id) | trimws(articles_cited$id) == "")
count_null_empty_id
# publisher: host_organization
unique_publishers <- unique(articles_cited$host_organization)
# number of publishers: ~1,600
num_unique_publishers <- length(unique_publishers)
# list top 50 publishers
print(unique_publishers[1:50])
# list NULL publishers = 5%
num_na <- sum(is.na(articles_cited$host_organization))
# Replace NA values and empty strings with "NA"
articles_cited$host_organization[is.na(articles_cited$host_organization) | trimws(articles_cited$host_organization) == ""] <- "NA"
# 1. First, showing all NA publisher: meaning publisher info is not available.
publisher_NA <- articles_cited[articles_cited$host_organization == "NA", ]
publisher_NA_id <-unique(publisher_NA$id)
# Check if any 'id' values are duplicated
any_duplicated_ids <- any(duplicated(publisher_NA$id))
# Not using unnect() because it flattens out every article per author, which creates a lot of duplicated info
library(jsonlite)
# Convert the 'author' dataframe to JSON for each row
publisher_NA <- publisher_NA %>%
mutate(author = sapply(author, function(x) toJSON(x)))
# Truncate only strings that exceed Excel's 32,767 character limit
publisher_NA <- publisher_NA %>%
mutate(across(where(is.character), ~ ifelse(nchar(.) > 32767, substr(., 1, 32767), .)))
# 2. Elsevier
publisher_elsevier <- articles_cited[articles_cited$host_organization == "Elsevier BV", ]
# 3. Springer
publisher_springer <- articles_cited[articles_cited$host_organization == "Springer Science+Business Media", ]
# Some of open source publishers
publisher_plos <-articles_cited[articles_cited$host_organization == "Public Library of Science", ]
publisher_aaas <-articles_cited[articles_cited$host_organization == "American Association for the Advancement of Science", ]
publisher_nature <-articles_cited[articles_cited$host_organization == "Nature Portfolio", ]
publisher_cdc <- articles_cited[articles_cited$host_organization == "Centers for Disease Control and Prevention", ]
publisher_ua <- articles_cited[articles_cited$host_organization == "University of Arizona", ]
publisher_uap <- articles_cited[articles_cited$host_organization == "University of Arizona Press", ]
#### Find duplicates and frequencies #####
# change DF here
df <-articles_cited
# Find the rows that are duplicated
duplicate_rows <- df[duplicated(df) | duplicated(df, fromLast = TRUE), ]
# Create a table to count the frequency of duplicated rows
#duplicate_frequency <- table(apply(duplicate_rows, 1, paste, collapse = "-"))
duplicate_frequency <- table(duplicate_rows$id)
# show more than 10 times cited. change "10" to any number
duplicate_ids <- names(duplicate_frequency[duplicate_frequency > 10])
duplicate_multi_cited_rows <- df[df$id %in% duplicate_ids, ]
duplicate_multi_cited_rows <- duplicate_multi_cited_rows %>%
mutate(across(where(is.character), ~ ifelse(nchar(.) > 32767, substr(., 1, 32767), .)))
# Remove duplicate rows from duplicate_multi_cited_rows
duplicate_multi_cited_rows_unique <- duplicate_multi_cited_rows[!duplicated(duplicate_multi_cited_rows), ]
write_xlsx(duplicate_multi_cited_rows, "citations/duplicate_multi_cited_2023.xlsx")
write_xlsx(duplicate_multi_cited_rows_unique, "citations/duplicate_multi_cited_unique_2023.xlsx")
# Save the modified dataset to Excel
write_xlsx(publisher_NA, "citations/publisher_NA_2023.xlsx")
write_xlsx(publisher_aaas, "citations/publisher_aaas_2023.xlsx")
write_xlsx(publisher_nature, "citations/publisher_nature_2023.xlsx")
write_xlsx(publisher_plos, "citations/publisher_plos_2023.xlsx")
######################################
######################################
### Function: To count journal occurrences for a given publisher
# @param: dataframe articles_cited
#          publisher_name
# return: journal and counts cited
count_journals_by_publisher <- function(articles_cited, publisher_name) {
# Filter rows where host_organization matches the specified publisher
publisher1 <-  articles_cited[grepl(publisher_name, articles_cited$host_organization, ignore.case = TRUE), ]
# Count the occurrences of each journal under the specified publisher
journal_counts <- table(publisher1$so)
journal_counts_df <- as.data.frame(journal_counts)
return(journal_counts_df)
}
publisher_name <- "Microbiology society"
journal_counts_df <- count_journals_by_publisher(articles_cited, publisher_name)
print(journal_counts_df)
publisher_name <- "Optica Publishing Group"
publisher1 <-  articles_cited[grepl(publisher_name, articles_cited$host_organization, ignore.case = TRUE), ]
journal_counts_df <- count_journals_by_publisher(articles_cited, publisher_name)
print(journal_counts_df)
publisher_name <- "Canadian Science Publishing"
publisher1 <-  articles_cited[grepl(publisher_name, articles_cited$host_organization, ignore.case = TRUE), ]
journal_counts_df <- count_journals_by_publisher(articles_cited, publisher_name)
print(journal_counts_df)
# Group by 'host_organization' and count the number of articles for each publisher
publisher_ranking <- articles_cited %>%
group_by(host_organization) %>%
summarise(article_count = n()) %>%
arrange(desc(article_count))
library(ggplot2)
top_20_publishers <- publisher_ranking %>% slice(1:20)
top_20_publishers$percentage <- (top_20_publishers$article_count / sum(top_20_publishers$article_count)) * 100
top_20_publishers$host_organization <- substr(top_20_publishers$host_organization, 1, 10)
# Bar plot for top 20 publishers
ggplot(top_20_publishers, aes(x = reorder(host_organization, -article_count), y = article_count)) +
geom_bar(stat = "identity", fill = "steelblue") +
# Real number (article count) inside the bar
geom_text(aes(label = article_count), vjust = 0.5, hjust = 1.2, size = 2.5, color = "white") +
# Adjust hjust and color for positioning inside
# Percentage outside the bar
geom_text(aes(label = sprintf("(%.1f%%)", percentage)), vjust = 0.5, hjust = -0.2, size = 3) +
# Adjust hjust for positioning outside
coord_flip() +  # Flip the axis for better readability
labs(x = "Publisher", y = "Number of Articles", title = "2019 UA Top 20 Publishers (Number of Articles Cited)") +
theme_minimal() +
theme(axis.text.y = element_text(size = 7))  # Reduce font size of publisher names
rank_top_cited_journals <- function(data, journal_col, top_n = 10) {
top_cited_journals <- data %>%
group_by(!!sym(journal_col)) %>%      # Group by the journal names (column provided by the user)
summarise(citation_count = n()) %>%   # Count the number of articles per journal
arrange(desc(citation_count)) %>%     # Sort by citation count in descending order
slice(1:top_n)                        # Select top 'n' journals
print(top_cited_journals, n = top_n)
}
rank_top_cited_journals(publisher_plos, "so")
rank_top_cited_journals(publisher_aaas, "so")
rank_top_cited_journals(publisher_nature, "so")
View(publisher_ua)
# Define the journal ID
journal_id <- "S161564205"
# Fetch works (articles) for the journal using the OpenAlex ID
articles <- oa_works(filter = list(host_venue.id = journal_id))
joural_articles <- oa_fetch(
entity = "works",
filter = list(host_venue.id = journal_id)
)
articles <- oa_fetch(
entity = "works",
filter = c("host_venue.id:S161564205")
)
articles <- oa_fetch(
entity = "works",
filter = c("locations.source.id:S161564205")
)
articles <- oa_fetch(
entity = "source",
identifier = journal_id
)
View(articles)
view (journal)
# find the journal info
journal <- oa_fetch(
entity = "source",
identifier = journal_id
)
view (journal)
joural_articles <- oa_fetch(
entity = "works",
identifier = journal_id
)
joural_articles <- oa_fetch(
entity = "works",
Source.ids = c("journal_id",)
)
joural_articles <- oa_fetch(
entity = "works",
Source.ids = c("journal_id")
)
joural_articles <- oa_fetch(
entity = "works",
so_id = c("journal_id")
)
joural_articles <- oa_fetch(
entity = "works",
ids = c("journal_id")
)
joural_articles <- oa_fetch(
entity = "works",
source.id  = c("journal_id")
)
test_article <- oa_fetch (
entity = "works",
identifier = "https://openalex.org/W2772802389"
)
View(test_article)
joural_articles <- oa_fetch(
entity = "works",
best_oa_location.host_organization  = c("University of Arizona")
)
joural_articles <- oa_fetch(
entity = "works",
best_oa_location.source.id  = c(journal_id)
)
View(joural_articles)
View(joural_articles)
View(joural_articles)
# Find if "so" contains # Find the unique values in the $so column
journal_articles_so_unique_values <- unique(journal_articles$so)
joural_articles <- oa_fetch(
entity = "works",
best_oa_location.source.id  = c(journal_id)
)
# Find if "so" contains # Find the unique values in the $so column
journal_articles_so_unique_values <- unique(journal_articles$so)
journal_articles <- oa_fetch(
entity = "works",
best_oa_location.source.id  = c(journal_id)
)
# Find if "so" contains # Find the unique values in the $so column
journal_articles_so_unique_values <- unique(journal_articles$so)
print(journal_articles_so_unique_values)
# Find if "so" contains # Find the unique values in the $so column
journal_articles_so_unique_values <- unique(tolower(journal_articles$so))
print(journal_articles_so_unique_values)
journal_articles_issn_unique_values <- unique(tolower(journal_articles$issn_l))
# Sort the data frame by $cited_by_count in descending order
journal_articles_cited_by_count <- journal_articles[order(-journal_articles$cited_by_count), ]
# View the sorted data
print(sorted_journal_articles)
# Sort the data frame by $cited_by_count in descending order
journal_articles_cited_by_count <- journal_articles[order(-journal_articles$cited_by_count), ]
# View the sorted data
print(sorted_journal_articles)
# View the sorted data
print(journal_articles_cited_by_count)
n
write_xlsx(journal_articles_cited_by_count, "JPE_articles_cited_by_count.xlsx")
write_xlsx(journal_articles_cited_by_count, "citations/JPE_articles_cited_by_count.xlsx")
View(joural_articles[[27]][[1]])
View(publisher_uap)
# Find if "title" contain any duplicates?
journal_articles_title_unique_values <- unique(tolower(journal_articles$title))
journal_articles_id_unique_values <- unique(tolower(journal_articles$id))
journal_articles_title_dup <- duplicate(tolower(journal_articles$title))
#####
journal_articles_title_dup <- duplicated(tolower(journal_articles$title))
articles <- oa_fetch(
entity = "works",
best_oa_location.source.id  = c(journal_id)
)
# Find if "id" contain any duplicates: 923/926
articles_id_unique_values <- unique(tolower(ariticles$id))
# Find if "id" contain any duplicates: 923/926
articles_id_unique_values <- unique(tolower(articles$id))
# Find if "title" contain any duplicates: 888 /926
articles_title_unique_values <- unique(tolower(articles$title))
#####
articles_title_dup <- duplicated(tolower(articles$title))
print(articles[articles_title_dup, ])
# Find if "so" contains # Find the unique values in the $so column:
articles_so_unique_values <- unique(tolower(articles$so))
# Find if multiple "issn_l" with the journal
articles_issn_unique_values <- unique(tolower(articles$issn_l))
# Sort the data frame by $cited_by_count in descending order
articles_cited_by_count <- articles[order(-articles$cited_by_count), ]
# View the sorted data
print(articles_cited_by_count)
#####
articles_title_dup <- duplicated(tolower(articles$title))
print(articles[articles_title_dup, ])
#####
lower_titles <-tolower(articles$title)
all_duplicate_titles <- articles[lower_titles %in% lower_titles[duplicated(lower_titles)], ]
print(all_duplicate_titles)
publisher_name <- "Microbiology society"
publisher_microbiology <- articles_cited[articles_cited$host_organization == publisher_name, ]
publisher_microbiology <- articles_cited[articles_cited$host_organization == "Microbiology society", ]
publisher_microbiology <- articles_cited[tolower(articles_cited$host_organization) == tolower(publisher_name), ]
publisher_elsevier <- articles_cited[tolower(articles_cited$host_organization) == "elsevier bv", ]
publisher_springer <- articles_cited[tolower(articles_cited$host_organization) == tolower("Springer Science+Business Media"), ]
publisher_plos <- articles_cited[tolower(articles_cited$host_organization) == tolower("Public Library of Science"), ]
publisher_aaas <- articles_cited[tolower(articles_cited$host_organization) == tolower("American Association for the Advancement of Science"), ]
publisher_nature <- articles_cited[tolower(articles_cited$host_organization) == tolower("Nature Portfolio"), ]
publisher_cdc <- articles_cited[tolower(articles_cited$host_organization) == tolower("Centers for Disease Control and Prevention"), ]
publisher_ua <- articles_cited[tolower(articles_cited$host_organization) == tolower("University of Arizona"), ]
publisher_uap <- articles_cited[tolower(articles_cited$host_organization) == tolower("University of Arizona Press"), ]
#### Find duplicates and frequencies #####
# change DF here
df <-articles_cited
# Save the modified dataset to Excel
write_xlsx(publisher_NA, "citations/publisher_NA_2023.xlsx")
write_xlsx(publisher_aaas, "citations/publisher_aaas_2023.xlsx")
write_xlsx(publisher_nature, "citations/publisher_nature_2023.xlsx")
write_xlsx(publisher_plos, "citations/publisher_plos_2023.xlsx")
write_xlsx(publisher_microbiology, "citations/publisher_microbiology_2023.xlsx")
# test case 2: cited 6 from microbiology
search_string <- "https://openalex.org/W2128159409"
indices_with_string <- which(sapply(org_works$referenced_works, function(x) search_string %in% x))
print(indices_with_string)
org_works[indices_with_string, ]$id
### Article test case 2: cited 6 from microbiology
# both final published version and pre-print existing: https://openalex.org/works/W4379795917 and https://openalex.org/W4319339791
search_string <- "https://openalex.org/W2128159409"
indices_with_string <- which(sapply(articles_cited$referenced_works, function(x) search_string %in% x))
print(indices_with_string)
org_works[indices_with_string, ]$id
articles_cited[indices_with_string, ]$id
# Publishers test case : cited 6 from microbiology
# both final published version and pre-print existing: https://openalex.org/works/W4379795917 and https://openalex.org/W4319339791
search_string <- "https://openalex.org/W2128159409"
indices_with_string <- which(sapply(publisher_microbiology$referenced_works, function(x) search_string %in% x))
print(indices_with_string)
View(publisher_microbiology)
indices_with_string <- which(sapply(publisher_microbiology$referenced_works, function(x) search_string %in% x))
print(indices_with_string)
### Article test case 2: cited 6 from microbiology
# both final published version and pre-print existing: https://openalex.org/works/W4379795917 and https://openalex.org/W4319339791
search_string <- "https://openalex.org/W2128159409"
indices_with_string <- which(sapply(articles_cited$referenced_works, function(x) search_string %in% x))
print(indices_with_string)
articles_cited[indices_with_string, ]$id
indices_with_string <- which(sapply(publisher_microbiology$referenced_works, function(x) search_string %in% x))
print(indices_with_string)
indices_with_string <- which(sapply(publisher_microbiology$id, function(x) search_string %in% x))
print(indices_with_string)
indices_with_string <- which(sapply(articles_cited$referenced_works, function(x) search_string %in% x))
print(indices_with_string)
articles_cited[indices_with_string, ]$id
### origin works: test case:
search_string <- "https://openalex.org/W2128159409"
indices_with_string <- which(sapply(org_works$referenced_works, function(x) search_string %in% x))
print(indices_with_string)
org_works[indices_with_string, ]$id
### Article cited: test case 2: cited 6 from microbiology
indices_with_string <- which(sapply(articles_cited, function(x) search_string %in% x))
print(indices_with_string)
articles_cited[indices_with_string, ]$id
Q
Q
Q
