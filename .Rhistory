# Rangelands: 782 (685 for best_oa)
# NA: 384
print(source_counts_df)
# Display the dataframe with counts
# JRM/REM:  4183+836 (3991 + 804 for best_oa)
# Rangelands: 782 (685 for best_oa)
# NA: 384
print(source_counts_df)
count2 <- UA_host3 %>% filter(grepl("10", url, ignore.case = TRUE)) %>% nrow()
count2 <- UA_host3 %>% filter(grepl("10.2458", url, ignore.case = TRUE)) %>% nrow()
ua_doi1 <- UA_host3 %>% filter(grepl("10.2458", url, ignore.case = TRUE))
View(ua_doi1)
rem_doi <- UA_host %>% filter(grepl("10.1016", url, ignore.case=TRUE))
View(rem_doi)
# All locations:
# count: 14903 (2024-07-11)
UA_host_all_location <- oa_fetch (
entity = "works",
locations.source.host_organization = "https://openalex.org/I138006243",
#count_only = TRUE
)
# Best OA location. find out host organization.
# count: 8394 (2024-07-11)
# This fetch will take a few minutes. So be patient .
UA_host_best_location <- oa_fetch(
entity = "works",
# UA campus repository ID does not work as a filter
best_oa_location.source.host_organization = "https://openalex.org/I138006243",
# If only need count, uncomment the below line for a quick run.
count_only = TRUE
# If only need some samples. using the below line.
# options = list(sample = 100, seed = 1)
)
df_rem <- subset(UA_host_all_location, grepl("Range Ecology and Management", so, ignore.case = TRUE))
# 4,183 from JRM.
count_jrm <- nrow(df_jrm) + nrow (df_rem)
# Filter the dataframe to get all rows where "so" is "journal of range management" (JRM) (case insenstive)
df_jrm <- subset(UA_host_all_location, grepl("journal of range management", so, ignore.case = TRUE))
df_rem <- subset(UA_host_all_location, grepl("Range Ecology and Management", so, ignore.case = TRUE))
#
count_jrm <- nrow(df_jrm) + nrow (df_rem)
df_rem <- subset(UA_host_all_location, grepl("Range Ecology and Management", so, ignore.case = TRUE)) # 432
#
count_jrm <- nrow(df_jrm) + nrow (df_rem)
# Count the number of occurrences of each unique value in the "source" column using dplyr
source_counts_df <- UA_host_all_location %>%
count(so, sort = TRUE)
# Display the dataframe with counts
# JRM/REM:  4183+836 (3991 + 804 for best_oa)
# Rangelands: 782 (685 for best_oa)
# NA: 384
print(source_counts_df)
### get metadata for a DOI
install.package("rcorssref")
### get metadata for a DOI
install.packages("rcorssref")
library(rcrossref)
### get metadata for a DOI
install.packages("rcrossref")
install.packages("rcrossref")
library(rcrossref)
# Function to get metadata for a DOI
get_doi_metadata <- function(doi) {
metadata <- cr_cn(doi, format = "citeproc-json")
return(metadata)
}
doi <- "https://doi.org/10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
### get metadata for a DOI
install.packages("rcrossref")
library(rcrossref)
# Function to get metadata for a DOI
get_doi_metadata <- function(doi) {
metadata <- cr_cn(doi, format = "citeproc-json")
return(metadata)
}
doi <- "https://doi.org/10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
# Function to get metadata for a DOI
get_doi_metadata <- function(doi) {
tryCatch({
metadata <- cr_cn(doi, format = "citeproc-json")
return(metadata)
}, error = function(e) {
message("Error retrieving metadata: ", e)
return(NULL)
})
}
doi <- "https://doi.org/10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
# Print the metadata
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
library (httr)
library(jsonlite)
# Function to get metadata for a DOI
get_doi_metadata <- function(doi) {
url <- paste0("https://api.crossref.org/works/", doi)
response <- GET(url)
if (status_code(response) == 200) {
metadata <- content(response, as = "text", encoding = "UTF-8")
metadata <- fromJSON(metadata, flatten = TRUE)
return(metadata$message)
} else {
message("Error retrieving metadata: ", status_code(response))
return(NULL)
}
}
# Example DOI
doi <- "10.2458/v24i1.22003"
# Get metadata for the example DOI
metadata <- get_doi_metadata(doi)
# Print the metadata
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
### Example 2:
doi <- "https://doi.org/10.2458/azu_jrm_v57i2_cox"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
# code generated by GPT-4
# check "is_oa_anywhere" field
# Set the filter for works that are open access anywhere
oa_filter <- "is_oa_anywhere:true"
# Search for works with the specified filter
results <- oa_search(
entity = "works",
filter = oa_filter,
per_page = 100  # Adjust per_page as needed
)
### Example 2:
doi <- "https://doi.org/10.2458/azu_jrm_v57i2_cox"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
View(metadata)
df_rem <- subset(UA_host_all_location, grepl("Range Ecology", so, ignore.case = TRUE)) # 432
# Filter the dataframe to get all rows where "so" is "journal of range management" (JRM) (case insenstive)
df_jrm <- subset(UA_host_all_location, grepl("journal of range management", so, ignore.case = TRUE))  #4,183
df_rem <- subset(UA_host_all_location, grepl("Range Ecology", so, ignore.case = TRUE)) # 432
# Count the number of occurrences of each unique value in the "source" column using dplyr
source_counts_df <- UA_host_all_location %>%
count(so, sort = TRUE)
# common libraries
library(openalexR)
packageVersion("openalexR")
library(dplyr)
library(ggplot2)
library(knitr)
library(testthat)
library(readr)
citation("openalexR")
# check to see if openAlexR has the latest entities in OpenAlex (OpenAlex updated its data model(Entities) in June 2023)
# Before April 2023: they are [1] "works"        "authors"      "venues"       "institutions" "concepts"
# If not, need to use openalexR developer's version
oa_entities()
options (openalexR.mailto="yhan@arizona.edu")
getwd()
setwd("/home/yhan/Documents/openalexR-test")
### Test data
test_data_UAL_authors     <- c("Yan Han", "Ellen Dubinski", "Fernando Rios", "Ahlam Saleh")
test_data_COM_authors     <- c("Phillip Kuo", "Bekir Tanriover", "Ahlam Saleh")
test_data_COM_Tucson_authors <- c("Che Carrie Liu", "Robert M. Aaronson", "Alexa Aasronson", "Mohammed Abbas", "")
test_data_science_authors <- c("Marek Rychlik", "Ali Bilgin", "Beichuan Zhang")
test_data_ischool_authors <- c("Hong Cui")
test_data_others          <- c("Leila Hudson", "Mona Hymel")
test_data_not_updated_authors <-c("Karen Padilla", "Haw-chih Tai")
test_data_affiliation <- c("University of Arizona")
test_data_year <- c("2022", "2021", "2020", "2012")
# Test works
works_from_dois <- oa_fetch(entity = "works", doi = c("https://doi.org/10.1093/ofid/ofac186", "https://doi.org/10.1007/s11192-013-1221-3"),  verbose = TRUE)
### Testing three datasets citations recall and precision using one article (published in 2022)
### OpenAlex: Precision
### OpenCitaitons: Precision 100%. Recall: 2/3
### Google scholar: Precision 100%. Recall 100%
works_from_dois <- oa_fetch(entity = "works", doi = c("https://doi.org/10.6017/ital.v40i1.12553"),  verbose = TRUE)
works_from_dois$cited_by_api_url
works_from_dois$ids
# All locations:
# count: 14903 (2024-07-11)
UA_host_all_location <- oa_fetch (
entity = "works",
locations.source.host_organization = "https://openalex.org/I138006243",
#count_only = TRUE
)
View(source_counts_df)
# common libraries
library(openalexR)
packageVersion("openalexR")
library(dplyr)
library(ggplot2)
library(knitr)
library(testthat)
library(readr)
citation("openalexR")
# check to see if openAlexR has the latest entities in OpenAlex (OpenAlex updated its data model(Entities) in June 2023)
# Before April 2023: they are [1] "works"        "authors"      "venues"       "institutions" "concepts"
# If not, need to use openalexR developer's version
oa_entities()
options (openalexR.mailto="yhan@arizona.edu")
getwd()
setwd("/home/yhan/Documents/openalexR-test")
### Test data
test_data_UAL_authors     <- c("Yan Han", "Ellen Dubinski", "Fernando Rios", "Ahlam Saleh")
test_data_COM_authors     <- c("Phillip Kuo", "Bekir Tanriover", "Ahlam Saleh")
test_data_COM_Tucson_authors <- c("Che Carrie Liu", "Robert M. Aaronson", "Alexa Aasronson", "Mohammed Abbas", "")
test_data_science_authors <- c("Marek Rychlik", "Ali Bilgin", "Beichuan Zhang")
test_data_ischool_authors <- c("Hong Cui")
test_data_others          <- c("Leila Hudson", "Mona Hymel")
test_data_not_updated_authors <-c("Karen Padilla", "Haw-chih Tai")
test_data_affiliation <- c("University of Arizona")
test_data_year <- c("2022", "2021", "2020", "2012")
# Best OA location. find out host organization.
# count: 8394 (2024-07-11)
# This fetch will take a few minutes. So be patient .
UA_host_best_location <- oa_fetch(
entity = "works",
# UA campus repository ID does not work as a filter
best_oa_location.source.host_organization = "https://openalex.org/I138006243",
# If only need count, uncomment the below line for a quick run.
count_only = TRUE
# If only need some samples. using the below line.
# options = list(sample = 100, seed = 1)
)
# Primary_location.source.host_organization.
# count: 24 (2024-07-11)
UA_host2 <- oa_fetch (
entity = "works",
primary_location.source.host_organization = "https://openalex.org/I138006243",
count_only = TRUE
)
############### Use campus repository source ID.
# no result using UA campus repository source ID.
UA_host5 <- oa_fetch (
entity = "works",
best_oa_location.source.host_organization = "https://openalex.org/S2764879211",
count_only = TRUE
)
UA_host6 <- oa_fetch (
entity = "works",
locations.source.host_organization = "https://openalex.org/S2764879211",
count_only = TRUE
)
# Filter the dataframe to get all rows where "so" is "journal of range management" (JRM) (case insenstive)
df_jrm <- subset(UA_host_all_location, grepl("journal of range management", so, ignore.case = TRUE))  #4,183
df_rem <- subset(UA_host_all_location, grepl("Range Ecology", so, ignore.case = TRUE)) # 432
# Count the number of occurrences of each unique value in the "source" column using dplyr
source_counts_df <- UA_host_all_location %>%
count(so, sort = TRUE)
# Display the dataframe with counts
# JRM/REM:  4183+836 (3991 + 804 for best_oa)
# Rangelands: 782 (685 for best_oa)
# NA: 384
print(source_counts_df)
#### Example 1: More like openalex pulled directly from crossref.
###  Campus repo:
doi <- "10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
library(rcrossref)
library (httr)
library(jsonlite)
# Function to get metadata for a DOI
get_doi_metadata <- function(doi) {
url <- paste0("https://api.crossref.org/works/", doi)
response <- GET(url)
if (status_code(response) == 200) {
metadata <- content(response, as = "text", encoding = "UTF-8")
metadata <- fromJSON(metadata, flatten = TRUE)
return(metadata$message)
} else {
message("Error retrieving metadata: ", status_code(response))
return(NULL)
}
}
#### Example 1: More like openalex pulled directly from crossref.
###  Campus repo:
doi <- "10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
### Example 3:
# https://doi.org/10.1038/ng.3667"
doi <- "https://doi.org/10.1038/ng.3667"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
#Load packages
library(openalexR)
library(tidyverse)
#adding email for the polite pool
options(openalexR.mailto="sylvia.orner@scranton.edu")
UofSworks.df <-oa_fetch(
entity="works",
institutions.ror=c("05xwb6v37"),
from_publication_date ="2013-01-01")
#Unnesting Authors
WorksUnique.df<-Works.df%>%
mutate(author=lapply(author, function(x){
names(x) <-paste0(names(x), "author")
return(x)
}))%>%
unnest(author)
Works.df <- UofSworks.df
#Unnesting Authors
WorksUnique.df<-Works.df%>%
mutate(author=lapply(author, function(x){
names(x) <-paste0(names(x), "author")
return(x)
}))%>%
unnest(author)
#Spot Checking U of S affiliated authors
WorksUnique.df%>%filter(institution_rorauthor== "https://ror.org/05xwb6v37")
View(WorksUnique.df)
View(Works.df)
View(Works.df[[4]][[1]])
#Removing Authors Unaffiliated Authors
WorksUnique.df<-WorksUnique.df%>%
filter(institution_rorauthor== "https://ror.org/05xwb6v37" & !str_detect(au_affiliation_rawauthor, "Penn"))
#Deduplicating list in case of multiple U of S authors associated with a single work
WorksUnique.df <-WorksUnique.df[!duplicated(WorksUnique.df$id),]
Works.df <- UofSworks.df
#Unnesting Authors
WorksUnique.df<-Works.df%>%
mutate(author=lapply(author, function(x){
names(x) <-paste0(names(x), "author")
return(x)
}))%>%
unnest(author)
#Spot Checking U of S affiliated authors
WorksUnique.df%>%filter(institution_rorauthor== "https://ror.org/05xwb6v37")
View(WorksUnique.df)
#Removing Authors Unaffiliated Authors
WorksUnique.df<-WorksUnique.df%>%
filter(institution_rorauthor== "https://ror.org/05xwb6v37" & !str_detect(au_affiliation_rawauthor, "Penn"))
#Deduplicating list in case of multiple U of S authors associated with a single work
WorksUnique.df <-WorksUnique.df[!duplicated(WorksUnique.df$id),]
#Unnesting Concepts
WorksByConcept.df<-UofSWorksUnique.df%>%
mutate(concepts=lapply(concepts, function(x){
names(x) <-paste0(names(x), "concepts")
return(x)
}))%>%
unnest(concepts)
UofSWorksUnique.df <-WorksUnique.df
#Unnesting Concepts
WorksByConcept.df<-UofSWorksUnique.df%>%
mutate(concepts=lapply(concepts, function(x){
names(x) <-paste0(names(x), "concepts")
return(x)
}))%>%
unnest(concepts)
#Filtering to root concepts(level = 0) with a concept score > 0.
#Filtering out humanities publications.
HumanitiesWorks.df <-WorksByConcept.df%>%
filter(display_nameconcepts %in% c("Political Science", "Philosophy", "Art", "Sociology", "History", "Psychology", "Economics", "Business")& scoreconcepts > 0 & levelconcepts == 0)
#Filtering out sciences publications
SciencesWorks.df <-WorksByConcept.df%>%
filter(display_nameconcepts %in% c("Mathematics", "Computer Science", "Geology", "Chemistry", "Biology", "Engineering", "Geography", "Materials Science", "Physics", "Environmental Sciences", "Medicine") & scoreconcepts >0 & levelconcepts == 0)
#Isolating and unpacking data in the referenced_works column
WorksCited <- as.list(unique(do.call(rbind,WorksUnique.df$referenced_works)))
#Removing any values of NA and any duplicate values
WorksCited <-unique(WorksCited) %>%discard(is.na)
#Creating an empty dataframe to store the results of the for loop.
WorksCited.df <-data.frame()
#Running the loop to retrieve works cited data (may take some time to run)
for(i in seq(1, length(WorksCited), by=50)){
batch_identifiers <-WorksCited[i:min(i+49, length(WorksCited))]
batch_data <-oa_fetch(identifier=batch_identifiers)
WorksCited.df<-rbind(WorksCited.df, batch_data)
}
#Saving dataframes for future reloading
UofSWorksUnique.df <-read_rds("UofSWorksUnique.Rds")
# Saving data frames to RDS files
saveRDS(UofSWorksUnique.df, "UofSWorksUnique.Rds")
saveRDS(WorksCited.df, "WorksCited.Rds")
#Saving dataframes for future reloading
UofSWorksUnique.df <-read_rds("UofSWorksUnique.Rds")
WorksCited.df <-read_rds("WorksCited.Rds")
#Mean and Median Publication Date of Works Cited
mean(WorksCited.df$publication_year)
median (WorksCited.df$publication_year)
#Charting Citations by Year of Publication
PubYears.df <-WorksCited.df %>% count(publication_year, sort = TRUE)
#Organizing Years into Decades for Easier Viewing
PubYears.df <-PubYears.df %>% mutate(Decade = (publication_year %/% 10)*10)
ggplot(PubYears.df, aes(x=reorder(Decade, -Decade), y=n))+
geom_bar(stat="identity")+
labs(x= "Decade of Publication", y= "Number of Publications Cited", title="Citation by Decade")
#Filtering Open Access Articles Published
UofSWorksUnique.df%>%count(is_oa==TRUE)
#Charting OA Publications through the years (excluding 2023 because the year is incomplete)
OAWorksbyYear.df <-UofSWorksUnique.df%>%
filter(is_oa==TRUE& publication_year!= "2023")%>%
count(publication_year, sort=TRUE)
ggplot(OAWorksbyYear.df, aes(x=reorder(publication_year,-publication_year),y=n, group=1))+
geom_line(color="black")+
geom_point()+
labs(x= "Year", y= "Number of Open Access Publications", title="Open Access Publications by Year")
#Filtering Publications by Year
WorksbyYear.df <-UofSWorksUnique.df%>%filter(publication_year!="2023") %>%count(publication_year, sort=TRUE)
#Calculating Mean and Median works per year.
median(WorksbyYear.df$n)
mean(WorksbyYear.df$n)
#Charting publications by year
ggplot(WorksbyYear.df, aes(x=reorder(publication_year,-publication_year),y=n, group=1))+
geom_line(color="black")+
geom_point()+
labs(x= "Year", y= "Number of Publications", title="Publications by Year")
#Counting publishers most frequently used by faculty
UofSWorksUnique.df%>%count(host_organization, sort=TRUE)
#Counting publishers most frequently cited by faculty
WorksCited.df%>%count(host_organization, sort=TRUE)
#Counting material types most frequently published by faculty
UofSWorksUnique.df%>% count(type, sort=TRUE)
#Counting material types most frequently cited by faculty
WorksCited.df%>% count(type, sort=TRUE)
#Counting Open Access works cited
WorksCited.df %>%count(is_oa, sort=TRUE)
#Counting Gold OA journals cited
WorksCited.df%>%filter(oa_status=="gold")%>%count(so, sort=TRUE)
#Loading U of S Journal Holdings
UofSJournals.df <-read_csv("UofSJournals.csv")
library(openalexR)
#library(dplyr)
library(tideverse)
#install.packages("dplyr")
install.packages("tideverse")
#install.packages("dplyr")
install.packages("tidyverse")
install.packages("tidyverse")
#library(dplyr)
library(tidyverse)
options (openalexR.mailto="yhan@arizona.edu")
getwd()
setwd("/home/yhan/Documents/UA-datasets/openalexR-test/")
# Banner-University Medical Center Tucson
UAUMC.df <-oa_fetch(
entity="works",
institutions.ror=c("02xbk5j62"),
from_publication_date ="2023-01-01")
# Just run 5-year data, 50,000 records
# It took about 15 mins to run.
UAworks1 <-oa_fetch(
entity="works",
institutions.ror=c("03m2x1q45"),
from_publication_date ="2019-01-01"
#count_only = TRUE
)
UAworks2 <-oa_fetch(
entity="works",
institutions.ror=c("03m2x1q45"),
from_publication_date ="2014-01-01",
count_only = TRUE
)
UAWorks_UAauthors.df<-UAWorks1.df%>%
mutate(author=lapply(author, function(x){
names(x) <-paste0(names(x), "author")
return(x)
}))%>%
unnest(author)
# 2. Data cleanup
# Flattening authors fields from the DF, and then extract UArizona authors only
UAworks_since2019 <- UAworks1
UAWorks_UAauthors.df<-UAworks_since2019.df%>%
mutate(author=lapply(author, function(x){
names(x) <-paste0(names(x), "author")
return(x)
}))%>%
unnest(author)
UAWorks_UAauthors<-UAworks_since2019%>%
mutate(author=lapply(author, function(x){
names(x) <-paste0(names(x), "author")
return(x)
}))%>%
unnest(author)
#Spot Checking U of S affiliated authors
UAWorks_UAauthors %>%filter(institution_rorauthor== "https://ror.org/03m2x1q45")
View(UAworks_since2019)
View(UAWorks_UAauthors)
#Spot Checking U of S affiliated authors
UAWorks_UAauthors %>%filter(institution_rorauthor== "https://ror.org/03m2x1q45")
UAWorks_UAauthors %>%filter(institution_rorauthor== "https://ror.org/03m2x1q45")
UAWorks_UAauthors2 %>%filter(institution_rorauthor== "https://ror.org/03m2x1q45")
UAWorks_UAauthors2 %>%filter(institution_rorauthor== "https://ror.org/03m2x1q45")
