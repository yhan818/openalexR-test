metadata <- get_doi_metadata(doi)
### get metadata for a DOI
install.packages("rcrossref")
library(rcrossref)
# Function to get metadata for a DOI
get_doi_metadata <- function(doi) {
metadata <- cr_cn(doi, format = "citeproc-json")
return(metadata)
}
doi <- "https://doi.org/10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
# Function to get metadata for a DOI
get_doi_metadata <- function(doi) {
tryCatch({
metadata <- cr_cn(doi, format = "citeproc-json")
return(metadata)
}, error = function(e) {
message("Error retrieving metadata: ", e)
return(NULL)
})
}
doi <- "https://doi.org/10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
# Print the metadata
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
library (httr)
library(jsonlite)
# Function to get metadata for a DOI
get_doi_metadata <- function(doi) {
url <- paste0("https://api.crossref.org/works/", doi)
response <- GET(url)
if (status_code(response) == 200) {
metadata <- content(response, as = "text", encoding = "UTF-8")
metadata <- fromJSON(metadata, flatten = TRUE)
return(metadata$message)
} else {
message("Error retrieving metadata: ", status_code(response))
return(NULL)
}
}
# Example DOI
doi <- "10.2458/v24i1.22003"
# Get metadata for the example DOI
metadata <- get_doi_metadata(doi)
# Print the metadata
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
### Example 2:
doi <- "https://doi.org/10.2458/azu_jrm_v57i2_cox"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
# code generated by GPT-4
# check "is_oa_anywhere" field
# Set the filter for works that are open access anywhere
oa_filter <- "is_oa_anywhere:true"
# Search for works with the specified filter
results <- oa_search(
entity = "works",
filter = oa_filter,
per_page = 100  # Adjust per_page as needed
)
### Example 2:
doi <- "https://doi.org/10.2458/azu_jrm_v57i2_cox"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
View(metadata)
df_rem <- subset(UA_host_all_location, grepl("Range Ecology", so, ignore.case = TRUE)) # 432
# Filter the dataframe to get all rows where "so" is "journal of range management" (JRM) (case insenstive)
df_jrm <- subset(UA_host_all_location, grepl("journal of range management", so, ignore.case = TRUE))  #4,183
df_rem <- subset(UA_host_all_location, grepl("Range Ecology", so, ignore.case = TRUE)) # 432
# Count the number of occurrences of each unique value in the "source" column using dplyr
source_counts_df <- UA_host_all_location %>%
count(so, sort = TRUE)
# common libraries
library(openalexR)
packageVersion("openalexR")
library(dplyr)
library(ggplot2)
library(knitr)
library(testthat)
library(readr)
citation("openalexR")
# check to see if openAlexR has the latest entities in OpenAlex (OpenAlex updated its data model(Entities) in June 2023)
# Before April 2023: they are [1] "works"        "authors"      "venues"       "institutions" "concepts"
# If not, need to use openalexR developer's version
oa_entities()
options (openalexR.mailto="yhan@arizona.edu")
getwd()
setwd("/home/yhan/Documents/openalexR-test")
### Test data
test_data_UAL_authors     <- c("Yan Han", "Ellen Dubinski", "Fernando Rios", "Ahlam Saleh")
test_data_COM_authors     <- c("Phillip Kuo", "Bekir Tanriover", "Ahlam Saleh")
test_data_COM_Tucson_authors <- c("Che Carrie Liu", "Robert M. Aaronson", "Alexa Aasronson", "Mohammed Abbas", "")
test_data_science_authors <- c("Marek Rychlik", "Ali Bilgin", "Beichuan Zhang")
test_data_ischool_authors <- c("Hong Cui")
test_data_others          <- c("Leila Hudson", "Mona Hymel")
test_data_not_updated_authors <-c("Karen Padilla", "Haw-chih Tai")
test_data_affiliation <- c("University of Arizona")
test_data_year <- c("2022", "2021", "2020", "2012")
# Test works
works_from_dois <- oa_fetch(entity = "works", doi = c("https://doi.org/10.1093/ofid/ofac186", "https://doi.org/10.1007/s11192-013-1221-3"),  verbose = TRUE)
### Testing three datasets citations recall and precision using one article (published in 2022)
### OpenAlex: Precision
### OpenCitaitons: Precision 100%. Recall: 2/3
### Google scholar: Precision 100%. Recall 100%
works_from_dois <- oa_fetch(entity = "works", doi = c("https://doi.org/10.6017/ital.v40i1.12553"),  verbose = TRUE)
works_from_dois$cited_by_api_url
works_from_dois$ids
# All locations:
# count: 14903 (2024-07-11)
UA_host_all_location <- oa_fetch (
entity = "works",
locations.source.host_organization = "https://openalex.org/I138006243",
#count_only = TRUE
)
View(source_counts_df)
# common libraries
library(openalexR)
packageVersion("openalexR")
library(dplyr)
library(ggplot2)
library(knitr)
library(testthat)
library(readr)
citation("openalexR")
# check to see if openAlexR has the latest entities in OpenAlex (OpenAlex updated its data model(Entities) in June 2023)
# Before April 2023: they are [1] "works"        "authors"      "venues"       "institutions" "concepts"
# If not, need to use openalexR developer's version
oa_entities()
options (openalexR.mailto="yhan@arizona.edu")
getwd()
setwd("/home/yhan/Documents/openalexR-test")
### Test data
test_data_UAL_authors     <- c("Yan Han", "Ellen Dubinski", "Fernando Rios", "Ahlam Saleh")
test_data_COM_authors     <- c("Phillip Kuo", "Bekir Tanriover", "Ahlam Saleh")
test_data_COM_Tucson_authors <- c("Che Carrie Liu", "Robert M. Aaronson", "Alexa Aasronson", "Mohammed Abbas", "")
test_data_science_authors <- c("Marek Rychlik", "Ali Bilgin", "Beichuan Zhang")
test_data_ischool_authors <- c("Hong Cui")
test_data_others          <- c("Leila Hudson", "Mona Hymel")
test_data_not_updated_authors <-c("Karen Padilla", "Haw-chih Tai")
test_data_affiliation <- c("University of Arizona")
test_data_year <- c("2022", "2021", "2020", "2012")
# Best OA location. find out host organization.
# count: 8394 (2024-07-11)
# This fetch will take a few minutes. So be patient .
UA_host_best_location <- oa_fetch(
entity = "works",
# UA campus repository ID does not work as a filter
best_oa_location.source.host_organization = "https://openalex.org/I138006243",
# If only need count, uncomment the below line for a quick run.
count_only = TRUE
# If only need some samples. using the below line.
# options = list(sample = 100, seed = 1)
)
# Primary_location.source.host_organization.
# count: 24 (2024-07-11)
UA_host2 <- oa_fetch (
entity = "works",
primary_location.source.host_organization = "https://openalex.org/I138006243",
count_only = TRUE
)
############### Use campus repository source ID.
# no result using UA campus repository source ID.
UA_host5 <- oa_fetch (
entity = "works",
best_oa_location.source.host_organization = "https://openalex.org/S2764879211",
count_only = TRUE
)
UA_host6 <- oa_fetch (
entity = "works",
locations.source.host_organization = "https://openalex.org/S2764879211",
count_only = TRUE
)
# Filter the dataframe to get all rows where "so" is "journal of range management" (JRM) (case insenstive)
df_jrm <- subset(UA_host_all_location, grepl("journal of range management", so, ignore.case = TRUE))  #4,183
df_rem <- subset(UA_host_all_location, grepl("Range Ecology", so, ignore.case = TRUE)) # 432
# Count the number of occurrences of each unique value in the "source" column using dplyr
source_counts_df <- UA_host_all_location %>%
count(so, sort = TRUE)
# Display the dataframe with counts
# JRM/REM:  4183+836 (3991 + 804 for best_oa)
# Rangelands: 782 (685 for best_oa)
# NA: 384
print(source_counts_df)
#### Example 1: More like openalex pulled directly from crossref.
###  Campus repo:
doi <- "10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
library(rcrossref)
library (httr)
library(jsonlite)
# Function to get metadata for a DOI
get_doi_metadata <- function(doi) {
url <- paste0("https://api.crossref.org/works/", doi)
response <- GET(url)
if (status_code(response) == 200) {
metadata <- content(response, as = "text", encoding = "UTF-8")
metadata <- fromJSON(metadata, flatten = TRUE)
return(metadata$message)
} else {
message("Error retrieving metadata: ", status_code(response))
return(NULL)
}
}
#### Example 1: More like openalex pulled directly from crossref.
###  Campus repo:
doi <- "10.2458/v24i1.22003"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
### Example 3:
# https://doi.org/10.1038/ng.3667"
doi <- "https://doi.org/10.1038/ng.3667"
metadata <- get_doi_metadata(doi)
if (!is.null(metadata)) {
print(metadata)
} else {
print("No metadata found for the given DOI.")
}
View(metadata)
library(openalexR)
#library(dplyr)
library(tidyverse)
# free unused obj to manage memory
rm(list=ls())
gc
options("max.print" = 100000)
options (openalexR.mailto="yhan@arizona.edu")
getwd()
setwd("/home/yhan/Documents/openalexR-test/")
# Load saved rds file into a data frame. This is year 2023 UA works
org_works <- readRDS("../works_cited.rds")
##### 2. Checking and verifying data
###### change this line only to update the right dataset.
ref_works <- org_works$referenced_works
# count how many "NA" in referenced_works col. ~ 10%-20%  of works contain "NA"
na_count <- sum(sapply(ref_works, function(x) is.logical(x) && is.na(x)))
print(na_count)
### 2.2 Combine all the references and do further data analysis
ref_works_combined <- unlist(ref_works, use.names = FALSE)
ref_works_combined <- ref_works_combined[!is.na(ref_works_combined)]  # Remove NA values
summary(ref_works_combined)
### 2.21 finding these duplicates, which mean the duplicates have been cited multiple times
# (probably more important to have these journals subscribed!)
# cited more: ~25% - 33%  (2023 data)
ref_works_more_cited <- ref_works_combined[duplicated(ref_works_combined)]
summary(ref_works_more_cited)
# 2.22 remove the duplicates for further processing.
# cited:
ref_works_cited <- unique(ref_works_combined)
summary(ref_works_cited)
# 3. Data cleanup.
# Flattening authors fields from the DF (multiple authors per work).
# 426,000 obs (multiple authors) from 50,400 obs (works)
org_works_since <- org_works
org_works_authors<-org_works_since%>%
mutate(author=lapply(author, function(x){
names(x) <-paste0(names(x), "author")
return(x)
}))%>%
unnest(author)
# After flattening, authors' fields (e.g. au_idauthor, institution_rorauthor) are displayed
colnames(org_works)
colnames(org_works_UAauthors)
colnames(org_works_UAauthors)
colnames(org_works_authors)
#### This is not 100% accurate because UArizona has child organization whose ROR is associated with an article. By filtering institution_rorauthor
# to UArizona's ROR, certain articles are left out!!!
org_works_UAauthors <- org_works_authors%>%filter(institution_rorauthor== "https://ror.org/03m2x1q45")
# 3. check work_cited
# 8.2 million
ref_works2 <- org_works_UAauthors$referenced_works
# Combine all the references and do further data analysis
ref_works_combined2 <- unlist(ref_works2, use.names = FALSE)
ref_works_combined2 <- ref_works_combined2[!is.na(ref_works_combined2)]  # Remove NA values
summary(ref_works_combined2)
# finding these duplicates, which mean the duplicates have been cited multiple times
# Cited multiple times: 65% ?
ref_works_more_cited2 <- ref_works_combined[duplicated(ref_works_combined2)]
summary(ref_works_more_cited2)
# 2.22 remove the duplicates for further processing.
# Cited:
ref_works_cited2 <- unique(ref_works_combined2)
summary(ref_works_cited2)
rm(ref_works_more_cited2)
gc()
# finding these duplicates, which mean the duplicates have been cited multiple times
# Cited multiple times: 65% ?
ref_works_more_cited2 <- ref_works_combined2[duplicated(ref_works_combined2)]
summary(ref_works_more_cited2)
# 2.22 remove the duplicates for further processing.
# Cited:
ref_works_cited2 <- unique(ref_works_combined2)
summary(ref_works_cited2)
# 3. check work_cited
# 8.2 million
org_ref_works <- org_works_UAauthors$referenced_works
rm(ref_works_cited2)
rm(ref_works_combined2)
rm(ref_works_more_cited2)
gc()
# Combine all the references and do further data analysis
org_works_combined <- unlist(org_ref_works, use.names = FALSE)
org_ref_works_combined <- org_ref_works_combined[!is.na(org_works_combined)]  # Remove NA values
# finding these duplicates, which mean the duplicates have been cited multiple times
# Cited multiple times: 65% ?
org_ref_works_more_cited <- org_ref_works_combined2[duplicated(org_ref_works_combined)]
# finding these duplicates, which mean the duplicates have been cited multiple times
# Cited multiple times: 65% ?
duplicated(org_ref_works_combined)
org_ref_works_combined <- org_ref_works_combined[!is.na(org_works_combined)]  # Remove NA values
# Combine all the references and do further data analysis
org_ref_works_combined <- unlist(org_ref_works, use.names = FALSE)
rm(org_works_combined)
org_ref_works_combined <- org_ref_works_combined[!is.na(org_works_combined)]  # Remove NA values
org_ref_works_combined <- org_ref_works_combined[!is.na(org_ref_works_combined)]  # Remove NA values
# finding these duplicates, which mean the duplicates have been cited multiple times
# Cited multiple times: 65% ?
duplicated(org_ref_works_combined)
org_ref_works_more_cited <- org_ref_works_combined[duplicated(org_ref_works_combined)]
# 2.22 remove the duplicates for further processing.
# Cited:
org_ref_works_cited <- unique(org_ref_works_combined)
# the real number of works cited
# 2023-current: 379,978
# 2022-current: 571,227. Note: This crashed R studio with 12 GB memeroy used. showing 560,000
num_of_works <- length(org_ref_works_cited)
#Creating an empty dataframe to store the results of the for loop.
works_cited_df <-data.frame()
View(org_works)
###################### Citation Analysis
# 372,000 works use 0.2 GB memory
articles_cited_df <- org_works
# Empty or NULL records
count_null_empty_id <- sum(is.na(articles_cited_df$id) | trimws(articles_cited_df$id) == "")
count_null_empty_id
missing_ids <- articles_cited_df[is.na(articles_cited_df$id) | trimws(articles_cited_df$id) == "", ]
print(missing_ids)
articles_cited_df <- articles_cited_df[!(is.na(articles_cited_df$id) | trimws(articles_cited_df$id) == ""), ]
# 1. Analyse journal usage
#  - remove any row whose col "issn_l" is empty or NULL
#  - 224,000 articles out of 241,000 works
temp_df <- articles_cited_df
articles_cited_df <- temp_df[!(is.na(temp_df$issn_l) | temp_df$issn_l == ""), ]
###
# Count the number of NA or empty strings in articles_cited_df
count_null_empty_id <- sum(is.na(articles_cited_df$id) | trimws(articles_cited_df$id) == "")
unique_publishers <- unique(articles_cited_df$host_organization)
# number of publishers: 1,600
num_unique_publishers <- length(unique_publishers)
# list top 50 publishers
print(unique_publishers[1:50])
# list NULL publishers. 18,000 / 372,000 works
num_na <- sum(is.na(articles_cited_df$host_organization))
print(num_na)
# showing all CDC's journals
CDC_journals <- data.frame()
CDC_journals <- articles_cited_df[articles_cited_df$host_organization == "Centers for Disease Control and Prevention", ]
# NULL records
count_null_empty_id <- sum(is.na(CDC_journals$id) | trimws(CDC_journals$id) == "")
Elsevier_journals <- data.frame()
Elsevier_journals <- articles_cited_df[articles_cited_df$host_organization == "Elsevier BV", ]
# NULL records
count_null_empty_id <- sum(is.na(Elsevier_journals$issn_l) | trimws(Elsevier_journals$issn_l) == "")
# NULL records
count_null_id <- sum(is.na(Elsevier_journals$issn_l) | trimws(Elsevier_journals$issn_l) == "")
View(Elsevier_journals)
###
# Count the number of NA or empty strings in articles_cited_df
count_null_empty_id <- sum(is.na(articles_cited_df$id) | trimws(articles_cited_df$id) == "")
CDC_journals <- articles_cited_df[articles_cited_df$host_organization == "Centers for Disease Control and Prevention", ]
# NULL records
count_null_empty_id <- sum(is.na(CDC_journals$id) | trimws(CDC_journals$id) == "")
# NULL records
count_null_empty_id <- sum(is.na(CDC_journals$id) | trimws(CDC_journals$id) == "")
# Number of rows in the original dataset and the filtered dataset
nrow(articles_cited_df)
nrow(CDC_journals)
CDC_journals <- articles_cited_df[trimws(articles_cited_df$host_organization) == "Centers for Disease Control and Prevention", ]
missing_ids_CDC <- CDC_journals[is.na(CDC_journals$id) | trimws(CDC_journals$id) == "", ]
View(missing_ids_CDC)  # View them in RStudio or print(missing_ids_CDC) to inspect
SPIE_journals <- data.frame()
SPIE_journals <- articles_cited_df[articles_cited_df$host_organization == "SPIE", ]
# NULL records
count_null_id <- sum(is.na(SPIE_journals$id) | trimws(SPIE_journals$id) == "")
Springer_journals <- data.frame()
Springer_journals <- articles_cited_df[articles_cited_df$host_organization == "Springer Science+Business Media", ]
count_null_id <- sum(is.na(Springer_journals$id))
count_null_id <- sum(is.na(articles_cited_df$id))
# Check rows in articles_cited_df where host_organization is "Springer Science+Business Media" and id is NA
springer_missing_ids <- articles_cited_df[articles_cited_df$host_organization == "Springer Science+Business Media" & is.na(articles_cited_df$id), ]
print(nrow(springer_missing_ids))  # This should match the 2425 you are seeing
# Check for rows with whitespace in 'id'
springer_missing_ids <- articles_cited_df[articles_cited_df$host_organization == "Springer Science+Business Media" & (is.na(articles_cited_df$id) | trimws(articles_cited_df$id) == ""), ]
print(nrow(springer_missing_ids))
unique(Springer_journals$id)
# Trim and normalize the host_organization column
articles_cited_df$host_organization <- trimws(articles_cited_df$host_organization)
# Now, try filtering for Springer Science+Business Media again
Springer_journals <- articles_cited_df[articles_cited_df$host_organization == "Springer Science+Business Media", ]
# Check for NA or empty 'id' in the filtered data
springer_missing_ids <- Springer_journals[is.na(Springer_journals$id) | trimws(Springer_journals$id) == "", ]
print(nrow(springer_missing_ids))
head(Springer_journals)
unique(Springer_journals$id)
# Trim and normalize the host_organization column
articles_cited_df$host_organization <- trimws(articles_cited_df$host_organization)
articles_cited_df$issn_l <- trimws(articles_cited_df$issn_l)
# Empty or NULL records
count_null_empty_id <- sum(is.na(articles_cited_df$id) | trimws(articles_cited_df$id) == "")
count_null_empty_id
articles_cited_df <- articles_cited_df[!(is.na(articles_cited_df$id) | trimws(articles_cited_df$id) == ""), ]
w
# Trim and normalize the host_organization column
articles_cited_df$host_organization <- trimws(articles_cited_df$host_organization)
articles_cited_df$issn_l <- trimws(articles_cited_df$issn_l)
# Empty or NULL records
count_null_empty_id <- sum(is.na(articles_cited_df$id) | trimws(articles_cited_df$id) == "")
count_null_empty_id
articles_cited_df <- articles_cited_df[!(is.na(articles_cited_df$id) | trimws(articles_cited_df$id) == ""), ]
# 1. Analyse journal usage
#  - remove any row whose col "issn_l" is empty or NULL
#  - 2023: 224,000 articles out of 241,000 works
temp_df <- articles_cited_df
articles_cited_df <- temp_df[!(is.na(temp_df$issn_l) | temp_df$issn_l == ""), ]
temp_df <- temp_df[!(is.na(temp_df$issn_l) | temp_df$issn_l == ""), ]
###
# Count the number of NA or empty strings in articles_cited_df
count_null_empty_id <- sum(is.na(articles_cited_df$id) | trimws(articles_cited_df$id) == "")
unique_publishers <- unique(articles_cited_df$host_organization)
# number of publishers: 1,600
num_unique_publishers <- length(unique_publishers)
# list top 50 publishers
print(unique_publishers[1:50])
# list NULL publishers. 18,000 / 372,000 works
num_na <- sum(is.na(articles_cited_df$host_organization))
print(num_na)
# Replace "NA" strings with actual NA values
articles_cited_df$host_organization[articles_cited_df$host_organization == "NA"] <- NA
# showing all CDC's journals
CDC_journals <- articles_cited_df[articles_cited_df$host_organization == "Centers for Disease Control and Prevention", ]
missing_ids_CDC <- CDC_journals[is.na(CDC_journals$id) | trimws(CDC_journals$id) == "", ]
View(missing_ids_CDC)  # View them in RStudio or print(missing_ids_CDC) to inspect
# NULL records
count_null_empty_id <- sum(is.na(CDC_journals$id) | trimws(CDC_journals$id) == "")
# Replace NA values and empty strings with "NA"
articles_cited_df$host_organization[is.na(articles_cited_df$host_organization) | trimws(articles_cited_df$host_organization) == ""] <- "NA"
# showing all CDC's journals
CDC_journals <- articles_cited_df[articles_cited_df$host_organization == "Centers for Disease Control and Prevention", ]
# NULL records
count_null_empty_id <- sum(is.na(CDC_journals$id) | trimws(CDC_journals$id) == "")
Springer_journals <- articles_cited_df[articles_cited_df$host_organization == "Springer Science+Business Media", ]
count_null_id <- sum(is.na(Springer_journals$id))
# 1. First, showing all NA publisher: meaning publisher info is not available.
NA_journals <- articles_cited_df[articles_cited_df$host_organization == "NA", ]
# Group by 'host_organization' and count the number of articles for each publisher
publisher_ranking <- articles_cited_df %>%
group_by(host_organization) %>%
summarise(article_count = n()) %>%
arrange(desc(article_count))
# View the top 50 publishers
top_50_publishers <- head(publisher_ranking, 50)
# Display the result
print(top_50_publishers)
View(top_50_publishers)
# Springer
Springer_journals <- articles_cited_df[articles_cited_df$host_organization == "Springer Science+Business Media", ]
count_null_id <- sum(is.na(Springer_journals$id))
# 1. First, showing all NA publisher: meaning publisher info is not available.
publisher_NA <- articles_cited_df[articles_cited_df$host_organization == "NA", ]
# Springer
publisher_springer <- articles_cited_df[articles_cited_df$host_organization == "Springer Science+Business Media", ]
count_null_id <- sum(is.na(publisher_springer$id))
# Elsevier
publisher_elsevier <- articles_cited_df[articles_cited_df$host_organization == "Elsevier BV", ]
# showing all CDC's journals
publisher_cdc <- articles_cited_df[articles_cited_df$host_organization == "Centers for Disease Control and Prevention", ]
# Group by 'host_organization' and count the number of articles for each publisher
publisher_ranking <- articles_cited_df %>%
group_by(host_organization) %>%
summarise(article_count = n()) %>%
arrange(desc(article_count))
# View the top 50 publishers
top_50_publishers <- head(publisher_ranking, 50)
# Display the result
print(top_50_publishers)
