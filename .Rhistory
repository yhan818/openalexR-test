View(metadata)
library(openalexR)
#library(dplyr)
library(tidyverse)
library(writexl)
# free unused obj to manage memory
rm(list=ls())
gc
options("max.print" = 100000)
options (openalexR.mailto="yhan@arizona.edu")
getwd()
setwd("/home/yhan/Documents/openalexR-test/")
org_works_2023 <- readRDS("../org_works_2023.rds")
# change working data here
org_works <- org_works_2023
##### 2. Checking and verifying data
###### change this line only to update the right dataset.
ref_works <- org_works$referenced_works
### 2.1 Checking Column referenced_works:  a list
class(ref_works)
# Find "NA" indexes
na_indices <- which(sapply(ref_works, function(x) is.logical(x) && is.na(x)))
# count how many "NA" in referenced_works col. ~ 15%-20%  of works contain "NA"
na_count <- sum(sapply(ref_works, function(x) is.logical(x) && is.na(x)))
print(na_count)
### 2.2 Combine all the references and do further data analysis
# Avg # of references per article: 38
# Year 2022 total references: 345,904
ref_works_combined <- unlist(ref_works, use.names = FALSE)
ref_works_combined <- ref_works_combined[!is.na(ref_works_combined)]  # Remove NA values
summary(ref_works_combined)
### 2.21 finding these duplicates, which mean the duplicates have been cited multiple times
# (probably more important to have these journals subscribed!)
# cited more: ~20% - 25%  (2022, 2023 UArizona data)
ref_works_more_cited <- ref_works_combined[duplicated(ref_works_combined)]
table(ref_works_more_cited)
############################################################
### 2.23 For Testing purpose: Trace back from the cited article -> $referenced_works -> original published article
# Find the index of multiple samples
which(ref_works_combined %in% c("https://openalex.org/W4292309267", "https://openalex.org/W621173178"))
# Find the index of "ref1" in the published works' referenced_works.
which(sapply(org_works$referenced_works, function(x) "https://openalex.org/W4292309267" %in% x))
# We can see the original works for samples
org_works[706, "id"]
org_works[779, "id"]
ref_works_cited <- unique(ref_works_combined)
summary(ref_works_cited)
##### 3. Data cleanup.
# Flattening authors fields from the DF (multiple authors per work).
# 426,000 obs (multiple authors) from 50,400 obs (works)
org_works_since <- org_works
#### Year 2022:
# -- org_works_authors: 75,222
# -- org_works_UAauthors: 16,432
# -- Org_ref_works_combined: 656,712
# -- Org_ref_works_cited: 249,629
#
org_works_authors<-org_works_since%>%
mutate(author=lapply(author, function(x){
names(x) <-paste0(names(x), "author")
return(x)
}))%>%
unnest(author)
#### Year 2022:
# -- org_works_authors: 75,222
# -- org_works_UAauthors: 16,432
# -- Org_ref_works_combined: 656,712
# -- Org_ref_works_cited: 249,629
#
org_works_authors<-org_works_since%>%
mutate(author=lapply(author, function(x){
names(x) <-paste0(names(x), "author")
return(x)
}))%>%
unnest(author)
library(dplyr)
#### Year 2022:
# -- org_works_authors: 75,222
# -- org_works_UAauthors: 16,432
# -- Org_ref_works_combined: 656,712
# -- Org_ref_works_cited: 249,629
#
org_works_authors<-org_works_since%>%
mutate(author=lapply(author, function(x){
names(x) <-paste0(names(x), "author")
return(x)
}))%>%
unnest(author)
library(tidyverse)
#### Year 2022:
# -- org_works_authors: 75,222
# -- org_works_UAauthors: 16,432
# -- Org_ref_works_combined: 656,712
# -- Org_ref_works_cited: 249,629
#
org_works_authors<-org_works_since%>%
mutate(author=lapply(author, function(x){
names(x) <-paste0(names(x), "author")
return(x)
}))%>%
unnest(author)
# After flattening, authors' fields (e.g. au_idauthor, institution_rorauthor) are displayed
colnames(org_works)
colnames(org_works_authors)
#### 3.3 TESTING!!!
# Then extract UArizona authors only
# 94,500 obs from 426,000 obs (UA authors only).
## https://openalex.org/A5033317672 Saurav Mallik (is at two affiliations for https://api.openalex.org/works/W4389611927. Harvard and University of Arizona)
### https://openalex.org/W4401226694 author Renu Malhotra has two affiliations.
oa_fetch_test1 <-oa_fetch( entity="works",  id="https://openalex.org/W4401226694")
oa_fetch_test1$author
view(oa_fetch_test1[[4]][[1]])
#### 3.3 TESTING!!!
# Then extract UArizona authors only
# 94,500 obs from 426,000 obs (UA authors only).
## https://openalex.org/A5033317672 Saurav Mallik (is at two affiliations for https://api.openalex.org/works/W4389611927. Harvard and University of Arizona)
### https://openalex.org/W4401226694 author Renu Malhotra has two affiliations.
oa_fetch_test1 <-oa_fetch( entity="works",  id="https://openalex.org/W4401226694")
library(openalexR)
library(dplyr)
library(tidyverse)
library(writexl)
#### 3.3 TESTING!!!
# Then extract UArizona authors only
# 94,500 obs from 426,000 obs (UA authors only).
## https://openalex.org/A5033317672 Saurav Mallik (is at two affiliations for https://api.openalex.org/works/W4389611927. Harvard and University of Arizona)
### https://openalex.org/W4401226694 author Renu Malhotra has two affiliations.
oa_fetch_test1 <-oa_fetch( entity="works",  id="https://openalex.org/W4401226694")
oa_fetch_test1$author
view(oa_fetch_test1[[4]][[1]])
oa_fetch_test2 <-oa_fetch_test2 <-oa_fetch( entity="authors",  id="https://openalex.org/A5003933592")
#### This is not 100% accurate because UArizona has child organization whose ROR is associated with an article. By filtering institution_rorauthor
# to UArizona's ROR, certain articles are left out!!!
# 2024-09: I am currently working with openAlexR developers to fix this.
org_works_UAauthors <- org_works_authors%>%filter(institution_rorauthor== "https://ror.org/03m2x1q45")
org_works_UAauthors_unique <- unique (org_works_UAauthors)
# 3.4 check work_cited
org_ref_works <- org_works_UAauthors$referenced_works
class(org_ref_works) # a list
# Combine all the references and do further data analysis.
# Giving avg ~ 38 references per work, the number of all the references'  will be 38x of the works.
org_ref_works_combined <- unlist(org_ref_works, use.names = FALSE)
org_ref_works_combined <- org_ref_works_combined[!is.na(org_ref_works_combined)]  # Remove NA values
# finding these duplicates, which mean the duplicates have been cited multiple times
# Cited multiple times = 65%
org_ref_works_more_cited <- org_ref_works_combined[duplicated(org_ref_works_combined)]
# 2.22 remove the duplicates for further processing. unique works cited ~38%
org_ref_works_cited <- unique(org_ref_works_combined)
# Find the indices of elements matching a pattern
matching_indices <- grep("https://openalex.org/W4401226694", org_ref_works)
print(matching_indices)  # Returns the index(es) of matching elements
matching_indices <- grep("https://openalex.org/W3203440266", ref_works )
print(matching_indices)  # Returns the index(es) of matching elements
#Running the loop to retrieve works cited data (may take someref_works_cited2#Running the loop to retrieve works cited data (may take some time to run)
# 1,000 for testing ONLY
num_of_works <- 1000
# the real number of works cited
# 2023-current: 379,978
# 2022-current: 571,227. Note: This crashed R studio with 12 GB memeroy used. showing 560,000
num_of_works <- length(org_ref_works_cited)
### Testing if a work is found.
search_string <- "https://openalex.org/W2919115771"
result <- lapply(org_ref_works, function(x) grep(search_string, x, value = TRUE))
print(result)
matches <- result[sapply(result, length) > 0]
print(matches)
indices <- which(sapply(org_ref_works, function(x) any(grepl(search_string, x))))
for (i in indices) {
cat("Index:", i, "\n")
cat("Element:\n", org_ref_works[[i]], "\n\n")
}
#Creating an empty dataframe to store the results of the for loop.
works_cited_df <-data.frame()
Works_Cited2_df <- works_cited_df
malaria_topic <- oa_fetch(entity = "topics", search = "malaria") %>%
filter(display_name == "Malaria") %>%
pull(id)
malaria_topic
#> [1] "https://openalex.org/T10091"
system.time({
res <- oa_fetch(
topics.id = malaria_topic,
entity = "works",
verbose = TRUE,
options = list(sample = 10000, seed = 1),
output = "list"
)
})
system.time({
batch_identifiers <-org_ref_works_cited[1:10000]
res <-oa_fetch(identifier=batch_identifiers,
entity = "works",
options= list(sample=10000, seed=1),
output="list")
})
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (idx in seq_along(range_i)) {
i <- range_i[idx]
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(entity="works", identifier=batch_identifiers,
options= list(sample=10000, seed=1), output="list")
works_cited_ls[[idx]] <- batch_data
}
})
range_i <- seq(1, num_of_works, by=fetch_number)
# fetch_number =50, 170 mins
# fetch_number = 1,000, 161 mins
library(data.table)
fetch_number <- 10000
num_of_works <- 10000
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (idx in seq_along(range_i)) {
i <- range_i[idx]
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(entity="works", identifier=batch_identifiers,
options= list(sample=10000, seed=1), output="list")
works_cited_ls[[idx]] <- batch_data
}
})
print(paste("fetch time: ", time_taken["elapsed"] / 60, "minutes"))
View(works_cited_ls)
fetch_number <- 50
num_of_works <- 10000
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
num_of_works <- 1000
fetch_number <- 50
num_of_works <- 1000
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (idx in seq_along(range_i)) {
i <- range_i[idx]
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(entity="works", identifier=batch_identifiers,
options= list(sample=10000, seed=1), output="list")
works_cited_ls[[idx]] <- batch_data
}
})
# fetch_number =50, 170 mins
# fetch_number = 1,000, 161 mins
library(data.table)
fetch_number <- 50
num_of_works <- 1000
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (idx in seq_along(range_i)) {
i <- range_i[idx]
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(entity="works", identifier=batch_identifiers,
options= list(sample=10000, seed=1), output="list")
works_cited_ls[[idx]] <- batch_data
}
})
print(paste("fetch time: ", time_taken["elapsed"] / 60, "minutes"))
fetch_number <- 50
num_of_works <- 1000
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (idx in seq_along(range_i)) {
i <- range_i[idx]
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(entity="works", identifier=batch_identifiers,
options= list(sample=fether_number, seed=1), output="list")
works_cited_ls[[idx]] <- batch_data
}
})
fetch_number <- 50
num_of_works <- 1000
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (idx in seq_along(range_i)) {
i <- range_i[idx]
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(entity="works", identifier=batch_identifiers,
options= list(sample=fetch_number, seed=1), output="list")
works_cited_ls[[idx]] <- batch_data
}
})
print(paste("fetch time: ", time_taken["elapsed"] / 60, "minutes"))
library(data.table)
fetch_number <- 50
num_of_works <- 10000
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (idx in seq_along(range_i)) {
i <- range_i[idx]
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(entity="works", identifier=batch_identifiers,
options= list(sample=fetch_number, seed=1), output="list")
works_cited_ls[[idx]] <- batch_data
}
})
print(paste("fetch time: ", time_taken["elapsed"] / 60, "minutes"))
View(works_cited_ls)
########################################
### Testing optimization of rbind and oa_fetch
### 2024-09-21: 10,000 works in R old version (4.1.2): 270 seconds
### 2024-09-23: 10,000 works in R latest version (4.4.1): 112 seconds
malaria_topic <- oa_fetch(entity = "topics", search = "malaria") %>%
filter(display_name == "Malaria") %>%
pull(id)
malaria_topic
#> [1] "https://openalex.org/T10091"
system.time({
res <- oa_fetch(
topics.id = malaria_topic,
entity = "works",
verbose = TRUE,
options = list(sample = 10000, seed = 1),
output = "list"
)
})
########################################
### Testing optimization of rbind and oa_fetch
### 2024-09-21: 10,000 works in R old version (4.1.2): 270 seconds
### 2024-09-23: 10,000 works in R latest version (4.4.1): 112 seconds
install.packages("profvis")
library(profvis)
fetch_number <- 1000
num_of_works <- 1000
fetch_number <- 1000
num_of_works <- 1000
# 2024-09-23: 10,000 works: 766 seconds
profvis({
batch_identifiers <-org_ref_works_cited[1:number_of_works]
res <-oa_fetch(identifier=batch_identifiers,
entity = "works",
options= list(sample=fetch_number, seed=1),
output="list")
})
batch_identifiers <-org_ref_works_cited[1:num_of_works]
res <-oa_fetch(identifier=batch_identifiers,
entity = "works",
options= list(sample=fetch_number, seed=1),
output="list")
fetch_number <- 100
num_of_works <- 100
# 2024-09-23: 10,000 works: 766 seconds
profvis({
batch_identifiers <-org_ref_works_cited[1:num_of_works]
res <-oa_fetch(identifier=batch_identifiers,
entity = "works",
options= list(sample=fetch_number, seed=1),
output="list")
})
# 2024-09-23: 10,000 works: 766 seconds
res <-list()
profvis({
batch_identifiers <-org_ref_works_cited[1:num_of_works]
res <-oa_fetch(identifier=batch_identifiers,
entity = "works",
options= list(sample=fetch_number, seed=1),
output="list")
})
fetch_number <- 10000
num_of_works <- 10000
# 2024-09-23: 10,000 works: 766 seconds
res <-list()
profvis({
batch_identifiers <-org_ref_works_cited[1:num_of_works]
res <-oa_fetch(identifier=batch_identifiers,
entity = "works",
options= list(sample=fetch_number, seed=1),
output="list")
})
fetch_number <- 100
num_of_works <- 10000
range_i <- seq(1, num_of_works, by=fetch_number)
works_cited_ls <- vector("list", length = length(range_i))
time_taken <-system.time({
for (idx in seq_along(range_i)) {
i <- range_i[idx]
batch_identifiers <-org_ref_works_cited[i:min(i+fetch_number-1, num_of_works)]
batch_data <-oa_fetch(entity="works", identifier=batch_identifiers,
options= list(sample=fetch_number, seed=1), output="list")
works_cited_ls[[idx]] <- batch_data
}
})
print(paste("fetch time: ", time_taken["elapsed"] / 60, "minutes"))
works_cited_df <- readRDS("../works_cited_2023.rds")
# the real number of works cited
# 2023-2024: 379,978
# 2022-2024: 571,227. Note: This crashed R studio with 12 GB memeroy used. showing 560,000
num_of_works <- length(org_ref_works_cited)
malaria_topic <- oa_fetch(entity = "topics", search = "malaria") %>%
filter(display_name == "Malaria") %>%
pull(id)
malaria_topic
#> [1] "https://openalex.org/T10091"
system.time({
res <- oa_fetch(
topics.id = malaria_topic,
entity = "works",
verbose = TRUE,
options = list(sample = 10000, seed = 1),
output = "list"
)
})
res <-list()
#profvis({
system.time({
batch_identifiers <-org_ref_works_cited[1:num_of_works]
res <-oa_fetch(identifier=batch_identifiers,
entity = "works",
options= list(sample=fetch_number, seed=1),
output="list")
})
fetch_number <- 10000
num_of_works <- 10000
res <-list()
#profvis({
system.time({
batch_identifiers <-org_ref_works_cited[1:num_of_works]
res <-oa_fetch(identifier=batch_identifiers,
entity = "works",
options= list(sample=fetch_number, seed=1),
output="list")
})
articles_cited_df <- works_cited_df
# 1. Analyse journal usage
#  - remove any row whose col "issn_l" is empty or NULL
#  - 2023: 224,572 articles out of 241,752 works
#  - 2022: 226,471 articles out of 243,452 works
articles_cited_df <- articles_cited_df[!(is.na(articles_cited_df$issn_l) | articles_cited_df$issn_l == ""), ]
nrow(articles_cited_df)
# Trim and normalize the host_organization column
articles_cited_df$host_organization <- trimws(articles_cited_df$host_organization)
articles_cited_df$issn_l <- trimws(articles_cited_df$issn_l)
# Empty or NULL records
count_null_empty_id <- sum(is.na(articles_cited_df$id) | trimws(articles_cited_df$id) == "")
count_null_empty_id
articles_cited_df <- articles_cited_df[!(is.na(articles_cited_df$id) | trimws(articles_cited_df$id) == ""), ]
# publisher: host_organization
unique_publishers <- unique(articles_cited_df$host_organization)
# number of publishers: ~1,600
num_unique_publishers <- length(unique_publishers)
# list top 50 publishers
print(unique_publishers[1:50])
# list NULL publishers = 5%
num_na <- sum(is.na(articles_cited_df$host_organization))
# Replace NA values and empty strings with "NA"
articles_cited_df$host_organization[is.na(articles_cited_df$host_organization) | trimws(articles_cited_df$host_organization) == ""] <- "NA"
# 1. First, showing all NA publisher: meaning publisher info is not available.
publisher_NA <- articles_cited_df[articles_cited_df$host_organization == "NA", ]
publisher_NA_id <-unique(publisher_NA$id)
# Check if any 'id' values are duplicated
any_duplicated_ids <- any(duplicated(publisher_NA$id))
# Not using unnect() because it flattens out every article per author, which creates a lot of duplicated info
library(jsonlite)
# Convert the 'author' dataframe to JSON for each row
publisher_NA <- publisher_NA %>%
mutate(author = sapply(author, function(x) toJSON(x)))
# Truncate only strings that exceed Excel's 32,767 character limit
publisher_NA <- publisher_NA %>%
mutate(across(where(is.character), ~ ifelse(nchar(.) > 32767, substr(., 1, 32767), .)))
# Save the modified dataset to Excel
write_xlsx(publisher_NA, "publisher_NA_2023.xlsx")
# 3.4 check work_cited
org_ref_works <- org_works_UAauthors$referenced_works
class(org_ref_works) # a list
# Combine all the references and do further data analysis.
# Giving avg ~ 38 references per work, the number of all the references'  will be 38x of the works.
org_ref_works_combined <-org_ref_works_cited
# Combine all the references and do further data analysis.
# Giving avg ~ 38 references per work, the number of all the references'  will be 38x of the works.
org_ref_works_combined <- unlist(org_ref_works, use.names = FALSE)
org_ref_works_combined <- org_ref_works_combined[!is.na(org_ref_works_combined)]  # Remove NA values
# the real number of works cited
# 2023-2024: 379,978
# 2022-2024: 571,227. Note: This crashed R studio with 12 GB memeroy used. showing 560,000
num_of_works <- length(org_ref_works_combined)
### Testing if a work is found.
search_string <- "https://openalex.org/W2919115771"
result <- lapply(org_ref_works, function(x) grep(search_string, x, value = TRUE))
print(result)
matches <- result[sapply(result, length) > 0]
print(matches)
indices <- which(sapply(org_ref_works, function(x) any(grepl(search_string, x))))
for (i in indices) {
cat("Index:", i, "\n")
cat("Element:\n", org_ref_works[[i]], "\n\n")
}
#Creating an empty dataframe to store the results of the for loop.
works_cited_df <-data.frame()
Works_Cited2_df <- works_cited_df
# 2.22 remove the duplicates for further processing. unique works cited ~38%
## Note: will need to de-dup to save time not to fetch duplicates. For Yr 2023, reducing from 658,000 to 248,000
org_ref_works_cited <- unique(org_ref_works_combined)
# the real number of works cited
# 2023-2024: 379,978
# 2022-2024: . Note: This crashed R studio with 12 GB memeroy used. showing 560,000
# 2023: 658,631
num_of_works <- length(org_ref_works_cited)
works_cited_df <- readRDS("../works_cited_2023.rds")
works_cited <- readRDS("../works_cited_2023.rds")
### get articles out of works
articles_cited <- works_cited[!is.na(works_cited$issn), ]
### get articles out of works
desc(works_cited)
### get articles out of works
summary(works_cited)
View(works_cited)
articles_cited <- works_cited[!is.na(works_cited$issn_l), ]
